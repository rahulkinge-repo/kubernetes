
==> Audit <==
|---------|--------------------------|----------|------------|---------|---------------------|---------------------|
| Command |           Args           | Profile  |    User    | Version |     Start Time      |      End Time       |
|---------|--------------------------|----------|------------|---------|---------------------|---------------------|
| start   | --driver=virtualbox      | minikube | rahulkinge | v1.34.0 | 06 Jan 25 14:08 EST |                     |
| start   | --driver=virtualbox      | minikube | rahulkinge | v1.34.0 | 06 Jan 25 14:20 EST |                     |
| start   | --driver=virtualbox      | minikube | rahulkinge | v1.34.0 | 06 Jan 25 14:26 EST |                     |
| start   | --driver=virtualbox      | minikube | rahulkinge | v1.34.0 | 06 Jan 25 14:33 EST |                     |
| delete  |                          | minikube | rahulkinge | v1.34.0 | 06 Jan 25 15:07 EST | 06 Jan 25 15:08 EST |
| start   | --driver=virtualbox      | minikube | rahulkinge | v1.34.0 | 06 Jan 25 15:08 EST |                     |
| delete  |                          | minikube | rahulkinge | v1.34.0 | 06 Jan 25 15:09 EST | 06 Jan 25 15:09 EST |
| start   | --driver=virtualbox      | minikube | rahulkinge | v1.34.0 | 06 Jan 25 15:10 EST |                     |
| start   | --driver=virtualbox      | minikube | rahulkinge | v1.34.0 | 08 Jan 25 11:13 EST |                     |
| start   | --driver=virtualbox      | minikube | rahulkinge | v1.34.0 | 08 Jan 25 11:14 EST |                     |
| delete  |                          | minikube | rahulkinge | v1.34.0 | 08 Jan 25 11:32 EST | 08 Jan 25 11:32 EST |
| start   | --driver=virtualbox      | minikube | rahulkinge | v1.34.0 | 08 Jan 25 11:35 EST |                     |
| delete  |                          | minikube | rahulkinge | v1.34.0 | 08 Jan 25 11:37 EST | 08 Jan 25 11:37 EST |
| start   | --driver=docker          | minikube | rahulkinge | v1.34.0 | 08 Jan 25 11:37 EST |                     |
| start   | --driver=docker          | minikube | rahulkinge | v1.34.0 | 08 Jan 25 11:42 EST | 08 Jan 25 11:44 EST |
| image   | ls                       | minikube | rahulkinge | v1.34.0 | 08 Jan 25 11:47 EST | 08 Jan 25 11:47 EST |
| service | hello-minikube           | minikube | rahulkinge | v1.34.0 | 08 Jan 25 11:53 EST |                     |
| delete  | --all                    | minikube | rahulkinge | v1.34.0 | 08 Jan 25 11:57 EST | 08 Jan 25 11:57 EST |
| start   | --driver=docker          | minikube | rahulkinge | v1.34.0 | 08 Jan 25 11:58 EST | 08 Jan 25 11:59 EST |
| start   |                          | minikube | rahulkinge | v1.34.0 | 13 Jan 25 12:55 EST | 13 Jan 25 12:59 EST |
| start   |                          | minikube | rahulkinge | v1.34.0 | 14 Jan 25 14:09 EST |                     |
| start   |                          | minikube | rahulkinge | v1.34.0 | 14 Jan 25 14:29 EST |                     |
| start   | --driver=docker          | minikube | rahulkinge | v1.34.0 | 14 Jan 25 14:38 EST |                     |
| delete  |                          | minikube | rahulkinge | v1.34.0 | 14 Jan 25 15:31 EST | 14 Jan 25 15:32 EST |
| start   | --driver=docker          | minikube | rahulkinge | v1.34.0 | 14 Jan 25 15:32 EST | 14 Jan 25 15:34 EST |
| start   | --driver=docker          | minikube | rahulkinge | v1.34.0 | 20 Feb 25 12:17 EST | 20 Feb 25 12:17 EST |
| service | myapp-service --url      | minikube | rahulkinge | v1.34.0 | 20 Feb 25 12:34 EST |                     |
| service | voting-service --url     | minikube | rahulkinge | v1.34.0 | 22 Feb 25 15:30 EST |                     |
| service | voting-app-service --url | minikube | rahulkinge | v1.34.0 | 22 Feb 25 15:30 EST |                     |
| service | voting-service --url     | minikube | rahulkinge | v1.34.0 | 22 Feb 25 15:31 EST |                     |
| service | voting-service --url     | minikube | rahulkinge | v1.34.0 | 22 Feb 25 15:32 EST |                     |
| service | voting-app-service --url | minikube | rahulkinge | v1.34.0 | 22 Feb 25 15:33 EST |                     |
| service | voting-service --url     | minikube | rahulkinge | v1.34.0 | 22 Feb 25 15:33 EST |                     |
| service | voting-service --url     | minikube | rahulkinge | v1.34.0 | 22 Feb 25 15:35 EST |                     |
| service | voting-service --url     | minikube | rahulkinge | v1.34.0 | 22 Feb 25 15:36 EST |                     |
| service | voting-service --url     | minikube | rahulkinge | v1.34.0 | 22 Feb 25 15:40 EST |                     |
| service | redis-service --url      | minikube | rahulkinge | v1.34.0 | 22 Feb 25 15:42 EST |                     |
| service | redis --url              | minikube | rahulkinge | v1.34.0 | 22 Feb 25 15:43 EST |                     |
| service | postgres-service --url   | minikube | rahulkinge | v1.34.0 | 22 Feb 25 15:43 EST |                     |
| service | db --url                 | minikube | rahulkinge | v1.34.0 | 22 Feb 25 15:44 EST |                     |
| service | result-service --url     | minikube | rahulkinge | v1.34.0 | 22 Feb 25 15:46 EST |                     |
| service | result-service --url     | minikube | rahulkinge | v1.34.0 | 22 Feb 25 15:47 EST |                     |
| service | result-service --url     | minikube | rahulkinge | v1.34.0 | 22 Feb 25 15:47 EST |                     |
| delete  | service result-service   | minikube | rahulkinge | v1.34.0 | 22 Feb 25 15:49 EST |                     |
| service | result-service --url     | minikube | rahulkinge | v1.34.0 | 22 Feb 25 15:54 EST |                     |
|---------|--------------------------|----------|------------|---------|---------------------|---------------------|


==> Last Start <==
Log file created at: 2025/02/20 12:17:05
Running on machine: Rahuls-MacBook-Air
Binary: Built with gc go1.23.1 for darwin/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0220 12:17:05.489585    1869 out.go:345] Setting OutFile to fd 1 ...
I0220 12:17:05.490734    1869 out.go:397] isatty.IsTerminal(1) = true
I0220 12:17:05.490740    1869 out.go:358] Setting ErrFile to fd 2...
I0220 12:17:05.490748    1869 out.go:397] isatty.IsTerminal(2) = true
I0220 12:17:05.491621    1869 root.go:338] Updating PATH: /Users/rahulkinge/.minikube/bin
I0220 12:17:05.496431    1869 out.go:352] Setting JSON to false
I0220 12:17:05.538026    1869 start.go:129] hostinfo: {"hostname":"Rahuls-MacBook-Air.local","uptime":1711,"bootTime":1740070114,"procs":517,"os":"darwin","platform":"darwin","platformFamily":"Standalone Workstation","platformVersion":"13.5.1","kernelVersion":"22.6.0","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"","hostId":"722292e5-c1ec-522b-bf03-297db787a6c6"}
W0220 12:17:05.538146    1869 start.go:137] gopshost.Virtualization returned error: not implemented yet
I0220 12:17:05.558495    1869 out.go:177] 😄  minikube v1.34.0 on Darwin 13.5.1
I0220 12:17:05.580258    1869 notify.go:220] Checking for updates...
I0220 12:17:05.580909    1869 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.31.0
I0220 12:17:05.583736    1869 driver.go:394] Setting default libvirt URI to qemu:///system
I0220 12:17:06.142305    1869 docker.go:123] docker version: linux-20.10.14:Docker Desktop 4.8.1 (78998)
I0220 12:17:06.142843    1869 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0220 12:17:07.220576    1869 cli_runner.go:217] Completed: docker system info --format "{{json .}}": (1.077623259s)
I0220 12:17:07.221185    1869 info.go:266] docker info: {ID:7NHK:AELK:ZR2G:36XO:5VLZ:FHFJ:3VGG:QX6M:A65Z:WKKE:4AE5:GGXL Containers:4 ContainersRunning:0 ContainersPaused:0 ContainersStopped:4 Images:4 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:50 OomKillDisable:false NGoroutines:50 SystemTime:2025-02-20 17:17:06.265808262 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:4 KernelVersion:5.10.104-linuxkit OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:2 MemTotal:4125892608 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[] ExperimentalBuild:false ServerVersion:20.10.14 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:3df54a852345ae127d1fa3092b95168e4a88e2f8 Expected:3df54a852345ae127d1fa3092b95168e4a88e2f8} RuncCommit:{ID:v1.0.3-0-gf46b6ba Expected:v1.0.3-0-gf46b6ba} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=default name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/local/lib/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.8.2] map[Name:compose Path:/usr/local/lib/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.5.0] map[Name:sbom Path:/usr/local/lib/docker/cli-plugins/docker-sbom SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scan Path:/usr/local/lib/docker/cli-plugins/docker-scan SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.17.0]] Warnings:<nil>}}
I0220 12:17:07.239956    1869 out.go:177] ✨  Using the docker driver based on existing profile
I0220 12:17:07.259838    1869 start.go:297] selected driver: docker
I0220 12:17:07.259880    1869 start.go:901] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0220 12:17:07.260092    1869 start.go:912] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0220 12:17:07.260509    1869 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0220 12:17:07.524521    1869 info.go:266] docker info: {ID:7NHK:AELK:ZR2G:36XO:5VLZ:FHFJ:3VGG:QX6M:A65Z:WKKE:4AE5:GGXL Containers:4 ContainersRunning:0 ContainersPaused:0 ContainersStopped:4 Images:4 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:50 OomKillDisable:false NGoroutines:50 SystemTime:2025-02-20 17:17:07.401937743 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:4 KernelVersion:5.10.104-linuxkit OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:2 MemTotal:4125892608 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[] ExperimentalBuild:false ServerVersion:20.10.14 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:3df54a852345ae127d1fa3092b95168e4a88e2f8 Expected:3df54a852345ae127d1fa3092b95168e4a88e2f8} RuncCommit:{ID:v1.0.3-0-gf46b6ba Expected:v1.0.3-0-gf46b6ba} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=default name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/local/lib/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.8.2] map[Name:compose Path:/usr/local/lib/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.5.0] map[Name:sbom Path:/usr/local/lib/docker/cli-plugins/docker-sbom SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scan Path:/usr/local/lib/docker/cli-plugins/docker-scan SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.17.0]] Warnings:<nil>}}
I0220 12:17:07.526505    1869 cni.go:84] Creating CNI manager for ""
I0220 12:17:07.527040    1869 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0220 12:17:07.527461    1869 start.go:340] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0220 12:17:07.565388    1869 out.go:177] 👍  Starting "minikube" primary control-plane node in "minikube" cluster
I0220 12:17:07.585734    1869 cache.go:121] Beginning downloading kic base image for docker with docker
I0220 12:17:07.605377    1869 out.go:177] 🚜  Pulling base image v0.0.45 ...
I0220 12:17:07.641544    1869 image.go:79] Checking for gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 in local docker daemon
I0220 12:17:07.642055    1869 preload.go:131] Checking if preload exists for k8s version v1.31.0 and runtime docker
I0220 12:17:07.642252    1869 preload.go:146] Found local preload: /Users/rahulkinge/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.31.0-docker-overlay2-amd64.tar.lz4
I0220 12:17:07.643283    1869 cache.go:56] Caching tarball of preloaded images
I0220 12:17:07.645317    1869 preload.go:172] Found /Users/rahulkinge/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.31.0-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0220 12:17:07.645354    1869 cache.go:59] Finished verifying existence of preloaded tar for v1.31.0 on docker
I0220 12:17:07.647124    1869 profile.go:143] Saving config to /Users/rahulkinge/.minikube/profiles/minikube/config.json ...
W0220 12:17:07.806587    1869 image.go:95] image gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 is of wrong architecture
I0220 12:17:07.806603    1869 cache.go:149] Downloading gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 to local cache
I0220 12:17:07.806963    1869 image.go:63] Checking for gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 in local cache directory
I0220 12:17:07.807782    1869 image.go:66] Found gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 in local cache directory, skipping pull
I0220 12:17:07.808227    1869 image.go:135] gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 exists in cache, skipping pull
I0220 12:17:07.808253    1869 cache.go:152] successfully saved gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 as a tarball
I0220 12:17:07.808264    1869 cache.go:162] Loading gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 from local cache
I0220 12:17:08.754513    1869 cache.go:164] successfully loaded and using gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 from cached tarball
I0220 12:17:08.755034    1869 cache.go:194] Successfully downloaded all kic artifacts
I0220 12:17:08.755167    1869 start.go:360] acquireMachinesLock for minikube: {Name:mk12bcd9669ba75c26b9d89e5551cfb73abbcfe2 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0220 12:17:08.755548    1869 start.go:364] duration metric: took 340.367µs to acquireMachinesLock for "minikube"
I0220 12:17:08.755598    1869 start.go:96] Skipping create...Using existing machine configuration
I0220 12:17:08.755613    1869 fix.go:54] fixHost starting: 
I0220 12:17:08.756334    1869 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0220 12:17:08.889889    1869 fix.go:112] recreateIfNeeded on minikube: state=Stopped err=<nil>
W0220 12:17:08.889948    1869 fix.go:138] unexpected machine state, will restart: <nil>
I0220 12:17:08.909195    1869 out.go:177] 🔄  Restarting existing docker container for "minikube" ...
I0220 12:17:08.927576    1869 cli_runner.go:164] Run: docker start minikube
I0220 12:17:09.708245    1869 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0220 12:17:09.906047    1869 kic.go:430] container "minikube" state is running.
I0220 12:17:09.908033    1869 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0220 12:17:10.046828    1869 profile.go:143] Saving config to /Users/rahulkinge/.minikube/profiles/minikube/config.json ...
I0220 12:17:10.047733    1869 machine.go:93] provisionDockerMachine start ...
I0220 12:17:10.047906    1869 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0220 12:17:10.186968    1869 main.go:141] libmachine: Using SSH client type: native
I0220 12:17:10.188534    1869 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1068a1dc0] 0x1068a4aa0 <nil>  [] 0s} 127.0.0.1 49442 <nil> <nil>}
I0220 12:17:10.188544    1869 main.go:141] libmachine: About to run SSH command:
hostname
I0220 12:17:10.192352    1869 main.go:141] libmachine: Error dialing TCP: ssh: handshake failed: EOF
I0220 12:17:13.389642    1869 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0220 12:17:13.391341    1869 ubuntu.go:169] provisioning hostname "minikube"
I0220 12:17:13.391879    1869 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0220 12:17:13.517465    1869 main.go:141] libmachine: Using SSH client type: native
I0220 12:17:13.517806    1869 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1068a1dc0] 0x1068a4aa0 <nil>  [] 0s} 127.0.0.1 49442 <nil> <nil>}
I0220 12:17:13.517815    1869 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0220 12:17:13.734304    1869 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0220 12:17:13.734896    1869 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0220 12:17:13.851169    1869 main.go:141] libmachine: Using SSH client type: native
I0220 12:17:13.851403    1869 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1068a1dc0] 0x1068a4aa0 <nil>  [] 0s} 127.0.0.1 49442 <nil> <nil>}
I0220 12:17:13.851417    1869 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0220 12:17:14.017551    1869 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0220 12:17:14.018512    1869 ubuntu.go:175] set auth options {CertDir:/Users/rahulkinge/.minikube CaCertPath:/Users/rahulkinge/.minikube/certs/ca.pem CaPrivateKeyPath:/Users/rahulkinge/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/Users/rahulkinge/.minikube/machines/server.pem ServerKeyPath:/Users/rahulkinge/.minikube/machines/server-key.pem ClientKeyPath:/Users/rahulkinge/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/Users/rahulkinge/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/Users/rahulkinge/.minikube}
I0220 12:17:14.018550    1869 ubuntu.go:177] setting up certificates
I0220 12:17:14.018574    1869 provision.go:84] configureAuth start
I0220 12:17:14.018752    1869 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0220 12:17:14.138356    1869 provision.go:143] copyHostCerts
I0220 12:17:14.140723    1869 exec_runner.go:144] found /Users/rahulkinge/.minikube/cert.pem, removing ...
I0220 12:17:14.141161    1869 exec_runner.go:203] rm: /Users/rahulkinge/.minikube/cert.pem
I0220 12:17:14.141384    1869 exec_runner.go:151] cp: /Users/rahulkinge/.minikube/certs/cert.pem --> /Users/rahulkinge/.minikube/cert.pem (1131 bytes)
I0220 12:17:14.142533    1869 exec_runner.go:144] found /Users/rahulkinge/.minikube/key.pem, removing ...
I0220 12:17:14.142539    1869 exec_runner.go:203] rm: /Users/rahulkinge/.minikube/key.pem
I0220 12:17:14.142634    1869 exec_runner.go:151] cp: /Users/rahulkinge/.minikube/certs/key.pem --> /Users/rahulkinge/.minikube/key.pem (1675 bytes)
I0220 12:17:14.143232    1869 exec_runner.go:144] found /Users/rahulkinge/.minikube/ca.pem, removing ...
I0220 12:17:14.143242    1869 exec_runner.go:203] rm: /Users/rahulkinge/.minikube/ca.pem
I0220 12:17:14.143349    1869 exec_runner.go:151] cp: /Users/rahulkinge/.minikube/certs/ca.pem --> /Users/rahulkinge/.minikube/ca.pem (1090 bytes)
I0220 12:17:14.143936    1869 provision.go:117] generating server cert: /Users/rahulkinge/.minikube/machines/server.pem ca-key=/Users/rahulkinge/.minikube/certs/ca.pem private-key=/Users/rahulkinge/.minikube/certs/ca-key.pem org=rahulkinge.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I0220 12:17:14.225383    1869 provision.go:177] copyRemoteCerts
I0220 12:17:14.226069    1869 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0220 12:17:14.226160    1869 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0220 12:17:14.354726    1869 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49442 SSHKeyPath:/Users/rahulkinge/.minikube/machines/minikube/id_rsa Username:docker}
I0220 12:17:14.474204    1869 ssh_runner.go:362] scp /Users/rahulkinge/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1090 bytes)
I0220 12:17:14.518796    1869 ssh_runner.go:362] scp /Users/rahulkinge/.minikube/machines/server.pem --> /etc/docker/server.pem (1192 bytes)
I0220 12:17:14.556861    1869 ssh_runner.go:362] scp /Users/rahulkinge/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I0220 12:17:14.595920    1869 provision.go:87] duration metric: took 575.33277ms to configureAuth
I0220 12:17:14.595940    1869 ubuntu.go:193] setting minikube options for container-runtime
I0220 12:17:14.596140    1869 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.31.0
I0220 12:17:14.596257    1869 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0220 12:17:14.731414    1869 main.go:141] libmachine: Using SSH client type: native
I0220 12:17:14.731737    1869 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1068a1dc0] 0x1068a4aa0 <nil>  [] 0s} 127.0.0.1 49442 <nil> <nil>}
I0220 12:17:14.731746    1869 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0220 12:17:14.914962    1869 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0220 12:17:14.914985    1869 ubuntu.go:71] root file system type: overlay
I0220 12:17:14.915661    1869 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I0220 12:17:14.915867    1869 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0220 12:17:15.041285    1869 main.go:141] libmachine: Using SSH client type: native
I0220 12:17:15.041609    1869 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1068a1dc0] 0x1068a4aa0 <nil>  [] 0s} 127.0.0.1 49442 <nil> <nil>}
I0220 12:17:15.041697    1869 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0220 12:17:15.229812    1869 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0220 12:17:15.230263    1869 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0220 12:17:15.346319    1869 main.go:141] libmachine: Using SSH client type: native
I0220 12:17:15.346566    1869 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1068a1dc0] 0x1068a4aa0 <nil>  [] 0s} 127.0.0.1 49442 <nil> <nil>}
I0220 12:17:15.346578    1869 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0220 12:17:15.525244    1869 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0220 12:17:15.525269    1869 machine.go:96] duration metric: took 5.477520721s to provisionDockerMachine
I0220 12:17:15.525727    1869 start.go:293] postStartSetup for "minikube" (driver="docker")
I0220 12:17:15.525762    1869 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0220 12:17:15.525963    1869 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0220 12:17:15.526074    1869 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0220 12:17:15.652840    1869 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49442 SSHKeyPath:/Users/rahulkinge/.minikube/machines/minikube/id_rsa Username:docker}
I0220 12:17:15.778721    1869 ssh_runner.go:195] Run: cat /etc/os-release
I0220 12:17:15.790117    1869 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0220 12:17:15.790151    1869 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0220 12:17:15.790163    1869 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0220 12:17:15.790170    1869 info.go:137] Remote host: Ubuntu 22.04.4 LTS
I0220 12:17:15.790586    1869 filesync.go:126] Scanning /Users/rahulkinge/.minikube/addons for local assets ...
I0220 12:17:15.791143    1869 filesync.go:126] Scanning /Users/rahulkinge/.minikube/files for local assets ...
I0220 12:17:15.791208    1869 start.go:296] duration metric: took 265.472365ms for postStartSetup
I0220 12:17:15.791690    1869 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0220 12:17:15.791768    1869 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0220 12:17:15.912250    1869 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49442 SSHKeyPath:/Users/rahulkinge/.minikube/machines/minikube/id_rsa Username:docker}
I0220 12:17:16.033988    1869 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0220 12:17:16.047956    1869 fix.go:56] duration metric: took 7.292334194s for fixHost
I0220 12:17:16.047988    1869 start.go:83] releasing machines lock for "minikube", held for 7.292410615s
I0220 12:17:16.048601    1869 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0220 12:17:16.172906    1869 ssh_runner.go:195] Run: cat /version.json
I0220 12:17:16.173063    1869 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0220 12:17:16.174182    1869 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0220 12:17:16.176385    1869 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0220 12:17:16.322728    1869 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49442 SSHKeyPath:/Users/rahulkinge/.minikube/machines/minikube/id_rsa Username:docker}
I0220 12:17:16.337511    1869 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49442 SSHKeyPath:/Users/rahulkinge/.minikube/machines/minikube/id_rsa Username:docker}
I0220 12:17:16.457797    1869 ssh_runner.go:195] Run: systemctl --version
I0220 12:17:16.831073    1869 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0220 12:17:16.849652    1869 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I0220 12:17:16.892820    1869 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I0220 12:17:16.893105    1869 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0220 12:17:16.912545    1869 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I0220 12:17:16.912575    1869 start.go:495] detecting cgroup driver to use...
I0220 12:17:16.912615    1869 detect.go:187] detected "cgroupfs" cgroup driver on host os
I0220 12:17:16.915113    1869 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0220 12:17:16.948435    1869 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.10"|' /etc/containerd/config.toml"
I0220 12:17:16.974332    1869 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0220 12:17:16.995490    1869 containerd.go:146] configuring containerd to use "cgroupfs" as cgroup driver...
I0220 12:17:16.995645    1869 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I0220 12:17:17.016037    1869 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0220 12:17:17.036267    1869 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0220 12:17:17.055757    1869 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0220 12:17:17.078293    1869 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0220 12:17:17.098216    1869 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0220 12:17:17.117665    1869 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I0220 12:17:17.141257    1869 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I0220 12:17:17.163136    1869 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0220 12:17:17.183990    1869 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0220 12:17:17.203956    1869 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0220 12:17:17.342054    1869 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0220 12:17:17.477511    1869 start.go:495] detecting cgroup driver to use...
I0220 12:17:17.477534    1869 detect.go:187] detected "cgroupfs" cgroup driver on host os
I0220 12:17:17.479285    1869 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0220 12:17:17.507292    1869 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0220 12:17:17.507556    1869 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0220 12:17:17.536190    1869 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0220 12:17:17.573529    1869 ssh_runner.go:195] Run: which cri-dockerd
I0220 12:17:17.587895    1869 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0220 12:17:17.616098    1869 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (190 bytes)
I0220 12:17:17.653192    1869 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0220 12:17:17.869589    1869 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0220 12:17:18.081192    1869 docker.go:574] configuring docker to use "cgroupfs" as cgroup driver...
I0220 12:17:18.081372    1869 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I0220 12:17:18.117002    1869 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0220 12:17:18.287644    1869 ssh_runner.go:195] Run: sudo systemctl restart docker
I0220 12:17:18.951874    1869 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I0220 12:17:18.971325    1869 ssh_runner.go:195] Run: sudo systemctl stop cri-docker.socket
I0220 12:17:18.991550    1869 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0220 12:17:19.011270    1869 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0220 12:17:19.147758    1869 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0220 12:17:19.273079    1869 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0220 12:17:19.404712    1869 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0220 12:17:19.441814    1869 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0220 12:17:19.465176    1869 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0220 12:17:19.599840    1869 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I0220 12:17:20.077942    1869 start.go:542] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0220 12:17:20.078605    1869 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0220 12:17:20.092009    1869 start.go:563] Will wait 60s for crictl version
I0220 12:17:20.092159    1869 ssh_runner.go:195] Run: which crictl
I0220 12:17:20.104725    1869 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0220 12:17:20.345160    1869 start.go:579] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  27.2.0
RuntimeApiVersion:  v1
I0220 12:17:20.345322    1869 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0220 12:17:20.537421    1869 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0220 12:17:20.604128    1869 out.go:235] 🐳  Preparing Kubernetes v1.31.0 on Docker 27.2.0 ...
I0220 12:17:20.605700    1869 cli_runner.go:164] Run: docker exec -t minikube dig +short host.docker.internal
I0220 12:17:21.114352    1869 network.go:96] got host ip for mount in container by digging dns: 192.168.65.2
I0220 12:17:21.114944    1869 ssh_runner.go:195] Run: grep 192.168.65.2	host.minikube.internal$ /etc/hosts
I0220 12:17:21.127737    1869 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.65.2	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0220 12:17:21.152960    1869 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0220 12:17:21.295821    1869 kubeadm.go:883] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I0220 12:17:21.296698    1869 preload.go:131] Checking if preload exists for k8s version v1.31.0 and runtime docker
I0220 12:17:21.296822    1869 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0220 12:17:21.354798    1869 docker.go:685] Got preloaded images: -- stdout --
nginx:latest
registry.k8s.io/kube-controller-manager:v1.31.0
registry.k8s.io/kube-apiserver:v1.31.0
registry.k8s.io/kube-scheduler:v1.31.0
registry.k8s.io/kube-proxy:v1.31.0
registry.k8s.io/etcd:3.5.15-0
registry.k8s.io/pause:3.10
registry.k8s.io/coredns/coredns:v1.11.1
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0220 12:17:21.354819    1869 docker.go:615] Images already preloaded, skipping extraction
I0220 12:17:21.355688    1869 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0220 12:17:21.398748    1869 docker.go:685] Got preloaded images: -- stdout --
nginx:latest
registry.k8s.io/kube-scheduler:v1.31.0
registry.k8s.io/kube-controller-manager:v1.31.0
registry.k8s.io/kube-apiserver:v1.31.0
registry.k8s.io/kube-proxy:v1.31.0
registry.k8s.io/etcd:3.5.15-0
registry.k8s.io/pause:3.10
registry.k8s.io/coredns/coredns:v1.11.1
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0220 12:17:21.399394    1869 cache_images.go:84] Images are preloaded, skipping loading
I0220 12:17:21.399789    1869 kubeadm.go:934] updating node { 192.168.49.2 8443 v1.31.0 docker true true} ...
I0220 12:17:21.400576    1869 kubeadm.go:946] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.31.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I0220 12:17:21.400786    1869 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0220 12:17:21.786183    1869 cni.go:84] Creating CNI manager for ""
I0220 12:17:21.786217    1869 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0220 12:17:21.786249    1869 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I0220 12:17:21.786294    1869 kubeadm.go:181] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.31.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0220 12:17:21.786551    1869 kubeadm.go:187] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.49.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.31.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%"
  nodefs.inodesFree: "0%"
  imagefs.available: "0%"
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0220 12:17:21.786692    1869 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.31.0
I0220 12:17:21.808877    1869 binaries.go:44] Found k8s binaries, skipping transfer
I0220 12:17:21.809024    1869 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0220 12:17:21.826224    1869 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I0220 12:17:21.864942    1869 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0220 12:17:21.899640    1869 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2150 bytes)
I0220 12:17:21.934855    1869 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0220 12:17:21.948778    1869 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0220 12:17:21.974548    1869 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0220 12:17:22.111100    1869 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0220 12:17:22.143966    1869 certs.go:68] Setting up /Users/rahulkinge/.minikube/profiles/minikube for IP: 192.168.49.2
I0220 12:17:22.144494    1869 certs.go:194] generating shared ca certs ...
I0220 12:17:22.144532    1869 certs.go:226] acquiring lock for ca certs: {Name:mkffa5d764b4a7ce6775ef7c53087fe6947cf1cf Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0220 12:17:22.144992    1869 certs.go:235] skipping valid "minikubeCA" ca cert: /Users/rahulkinge/.minikube/ca.key
I0220 12:17:22.145493    1869 certs.go:235] skipping valid "proxyClientCA" ca cert: /Users/rahulkinge/.minikube/proxy-client-ca.key
I0220 12:17:22.145719    1869 certs.go:256] generating profile certs ...
I0220 12:17:22.146191    1869 certs.go:359] skipping valid signed profile cert regeneration for "minikube-user": /Users/rahulkinge/.minikube/profiles/minikube/client.key
I0220 12:17:22.146749    1869 certs.go:359] skipping valid signed profile cert regeneration for "minikube": /Users/rahulkinge/.minikube/profiles/minikube/apiserver.key.7fb57e3c
I0220 12:17:22.147194    1869 certs.go:359] skipping valid signed profile cert regeneration for "aggregator": /Users/rahulkinge/.minikube/profiles/minikube/proxy-client.key
I0220 12:17:22.147976    1869 certs.go:484] found cert: /Users/rahulkinge/.minikube/certs/ca-key.pem (1675 bytes)
I0220 12:17:22.148280    1869 certs.go:484] found cert: /Users/rahulkinge/.minikube/certs/ca.pem (1090 bytes)
I0220 12:17:22.148354    1869 certs.go:484] found cert: /Users/rahulkinge/.minikube/certs/cert.pem (1131 bytes)
I0220 12:17:22.148998    1869 certs.go:484] found cert: /Users/rahulkinge/.minikube/certs/key.pem (1675 bytes)
I0220 12:17:22.160574    1869 ssh_runner.go:362] scp /Users/rahulkinge/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0220 12:17:22.214053    1869 ssh_runner.go:362] scp /Users/rahulkinge/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I0220 12:17:22.298667    1869 ssh_runner.go:362] scp /Users/rahulkinge/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0220 12:17:22.347226    1869 ssh_runner.go:362] scp /Users/rahulkinge/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I0220 12:17:22.404300    1869 ssh_runner.go:362] scp /Users/rahulkinge/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I0220 12:17:22.448260    1869 ssh_runner.go:362] scp /Users/rahulkinge/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I0220 12:17:22.494019    1869 ssh_runner.go:362] scp /Users/rahulkinge/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0220 12:17:22.550704    1869 ssh_runner.go:362] scp /Users/rahulkinge/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1675 bytes)
I0220 12:17:22.601302    1869 ssh_runner.go:362] scp /Users/rahulkinge/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0220 12:17:22.674423    1869 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0220 12:17:22.875245    1869 ssh_runner.go:195] Run: openssl version
I0220 12:17:22.940422    1869 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0220 12:17:22.986051    1869 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0220 12:17:23.007598    1869 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 Jan  8 16:44 /usr/share/ca-certificates/minikubeCA.pem
I0220 12:17:23.007761    1869 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0220 12:17:23.034871    1869 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0220 12:17:23.061227    1869 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I0220 12:17:23.084907    1869 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I0220 12:17:23.129290    1869 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I0220 12:17:23.147345    1869 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I0220 12:17:23.194591    1869 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I0220 12:17:23.223761    1869 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I0220 12:17:23.272543    1869 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I0220 12:17:23.306595    1869 kubeadm.go:392] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0220 12:17:23.306981    1869 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0220 12:17:23.380797    1869 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0220 12:17:23.408411    1869 kubeadm.go:408] found existing configuration files, will attempt cluster restart
I0220 12:17:23.408862    1869 kubeadm.go:593] restartPrimaryControlPlane start ...
I0220 12:17:23.409079    1869 ssh_runner.go:195] Run: sudo test -d /data/minikube
I0220 12:17:23.441818    1869 kubeadm.go:130] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I0220 12:17:23.442017    1869 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0220 12:17:23.783476    1869 kubeconfig.go:125] found "minikube" server: "https://127.0.0.1:50485"
I0220 12:17:23.785596    1869 kubeconfig.go:47] verify endpoint returned: got: 127.0.0.1:50485, want: 127.0.0.1:49441
I0220 12:17:23.794435    1869 kubeconfig.go:62] /Users/rahulkinge/.kube/config needs updating (will repair): [kubeconfig needs server address update]
I0220 12:17:23.795937    1869 lock.go:35] WriteFile acquiring /Users/rahulkinge/.kube/config: {Name:mkad5058222fcc8517af7d663b8fb5d8d887155f Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0220 12:17:23.905119    1869 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I0220 12:17:23.941346    1869 kubeadm.go:630] The running cluster does not require reconfiguration: 127.0.0.1
I0220 12:17:23.941420    1869 kubeadm.go:597] duration metric: took 532.546163ms to restartPrimaryControlPlane
I0220 12:17:23.941443    1869 kubeadm.go:394] duration metric: took 635.178409ms to StartCluster
I0220 12:17:23.945595    1869 settings.go:142] acquiring lock: {Name:mk00aaa2e068236a459f95ed7b59c82112790c9c Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0220 12:17:23.946086    1869 settings.go:150] Updating kubeconfig:  /Users/rahulkinge/.kube/config
I0220 12:17:23.949211    1869 lock.go:35] WriteFile acquiring /Users/rahulkinge/.kube/config: {Name:mkad5058222fcc8517af7d663b8fb5d8d887155f Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0220 12:17:23.955745    1869 start.go:235] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I0220 12:17:23.955911    1869 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.31.0
I0220 12:17:23.959344    1869 addons.go:507] enable addons start: toEnable=map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volcano:false volumesnapshots:false yakd:false]
I0220 12:17:23.963494    1869 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0220 12:17:23.963508    1869 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0220 12:17:23.963551    1869 addons.go:234] Setting addon storage-provisioner=true in "minikube"
W0220 12:17:23.963558    1869 addons.go:243] addon storage-provisioner should already be in state true
I0220 12:17:23.965595    1869 host.go:66] Checking if "minikube" exists ...
I0220 12:17:23.967689    1869 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0220 12:17:23.972685    1869 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0220 12:17:23.972923    1869 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0220 12:17:23.995250    1869 out.go:177] 🔎  Verifying Kubernetes components...
I0220 12:17:24.014729    1869 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0220 12:17:24.303514    1869 out.go:177]     ▪ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0220 12:17:24.329078    1869 addons.go:234] Setting addon default-storageclass=true in "minikube"
W0220 12:17:24.329095    1869 addons.go:243] addon default-storageclass should already be in state true
I0220 12:17:24.329124    1869 host.go:66] Checking if "minikube" exists ...
I0220 12:17:24.330079    1869 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0220 12:17:24.346194    1869 addons.go:431] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0220 12:17:24.346226    1869 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0220 12:17:24.346452    1869 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0220 12:17:25.007939    1869 addons.go:431] installing /etc/kubernetes/addons/storageclass.yaml
I0220 12:17:25.007974    1869 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0220 12:17:25.008222    1869 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0220 12:17:25.008510    1869 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49442 SSHKeyPath:/Users/rahulkinge/.minikube/machines/minikube/id_rsa Username:docker}
I0220 12:17:25.275776    1869 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49442 SSHKeyPath:/Users/rahulkinge/.minikube/machines/minikube/id_rsa Username:docker}
I0220 12:17:25.476022    1869 ssh_runner.go:235] Completed: sudo systemctl daemon-reload: (1.461256045s)
I0220 12:17:25.476221    1869 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0220 12:17:25.878376    1869 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0220 12:17:26.312127    1869 api_server.go:52] waiting for apiserver process to appear ...
I0220 12:17:26.312920    1869 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0220 12:17:26.430365    1869 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0220 12:17:26.431302    1869 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0220 12:17:26.812616    1869 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0220 12:17:27.824265    1869 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: (1.392907947s)
W0220 12:17:27.824308    1869 addons.go:457] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0220 12:17:27.824494    1869 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: (1.394105834s)
W0220 12:17:27.824510    1869 addons.go:457] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0220 12:17:27.824616    1869 ssh_runner.go:235] Completed: sudo pgrep -xnf kube-apiserver.*minikube.*: (1.010619279s)
I0220 12:17:27.824822    1869 retry.go:31] will retry after 271.275849ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0220 12:17:27.824895    1869 retry.go:31] will retry after 269.405407ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0220 12:17:27.824960    1869 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0220 12:17:28.095759    1869 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I0220 12:17:28.096906    1869 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I0220 12:17:28.313310    1869 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0220 12:17:28.592917    1869 addons.go:457] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0220 12:17:28.592942    1869 retry.go:31] will retry after 269.054657ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
W0220 12:17:28.606669    1869 addons.go:457] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0220 12:17:28.606692    1869 retry.go:31] will retry after 441.103903ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0220 12:17:28.606749    1869 api_server.go:72] duration metric: took 4.65093787s to wait for apiserver process to appear ...
I0220 12:17:28.606759    1869 api_server.go:88] waiting for apiserver healthz status ...
I0220 12:17:28.607335    1869 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:49441/healthz ...
I0220 12:17:28.613629    1869 api_server.go:269] stopped: https://127.0.0.1:49441/healthz: Get "https://127.0.0.1:49441/healthz": EOF
I0220 12:17:28.863105    1869 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I0220 12:17:29.049806    1869 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I0220 12:17:29.107115    1869 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:49441/healthz ...
I0220 12:17:29.120359    1869 api_server.go:269] stopped: https://127.0.0.1:49441/healthz: Get "https://127.0.0.1:49441/healthz": EOF
I0220 12:17:29.607309    1869 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:49441/healthz ...
I0220 12:17:29.614517    1869 api_server.go:269] stopped: https://127.0.0.1:49441/healthz: Get "https://127.0.0.1:49441/healthz": EOF
W0220 12:17:29.621684    1869 addons.go:457] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0220 12:17:29.621734    1869 retry.go:31] will retry after 469.873451ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
W0220 12:17:29.647839    1869 addons.go:457] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0220 12:17:29.647862    1869 retry.go:31] will retry after 735.90155ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0220 12:17:30.092437    1869 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I0220 12:17:30.106917    1869 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:49441/healthz ...
I0220 12:17:30.109726    1869 api_server.go:269] stopped: https://127.0.0.1:49441/healthz: Get "https://127.0.0.1:49441/healthz": EOF
I0220 12:17:30.385657    1869 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I0220 12:17:30.610018    1869 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:49441/healthz ...
I0220 12:17:34.637177    1869 api_server.go:279] https://127.0.0.1:49441/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0220 12:17:34.637238    1869 api_server.go:103] status: https://127.0.0.1:49441/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0220 12:17:34.637271    1869 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:49441/healthz ...
I0220 12:17:34.791329    1869 api_server.go:279] https://127.0.0.1:49441/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0220 12:17:34.791356    1869 api_server.go:103] status: https://127.0.0.1:49441/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0220 12:17:35.106992    1869 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:49441/healthz ...
I0220 12:17:35.145500    1869 api_server.go:279] https://127.0.0.1:49441/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0220 12:17:35.145567    1869 api_server.go:103] status: https://127.0.0.1:49441/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0220 12:17:35.606951    1869 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:49441/healthz ...
I0220 12:17:35.633121    1869 api_server.go:279] https://127.0.0.1:49441/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0220 12:17:35.633179    1869 api_server.go:103] status: https://127.0.0.1:49441/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0220 12:17:36.107214    1869 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:49441/healthz ...
I0220 12:17:36.265631    1869 api_server.go:279] https://127.0.0.1:49441/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0220 12:17:36.265694    1869 api_server.go:103] status: https://127.0.0.1:49441/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0220 12:17:36.607194    1869 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:49441/healthz ...
I0220 12:17:36.703842    1869 api_server.go:279] https://127.0.0.1:49441/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0220 12:17:36.703876    1869 api_server.go:103] status: https://127.0.0.1:49441/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0220 12:17:37.107213    1869 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:49441/healthz ...
I0220 12:17:37.145413    1869 api_server.go:279] https://127.0.0.1:49441/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0220 12:17:37.145449    1869 api_server.go:103] status: https://127.0.0.1:49441/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0220 12:17:37.607581    1869 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:49441/healthz ...
I0220 12:17:37.655718    1869 api_server.go:279] https://127.0.0.1:49441/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0220 12:17:37.655739    1869 api_server.go:103] status: https://127.0.0.1:49441/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0220 12:17:38.109080    1869 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:49441/healthz ...
I0220 12:17:38.152420    1869 api_server.go:279] https://127.0.0.1:49441/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0220 12:17:38.152468    1869 api_server.go:103] status: https://127.0.0.1:49441/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0220 12:17:38.608598    1869 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:49441/healthz ...
I0220 12:17:38.627534    1869 api_server.go:279] https://127.0.0.1:49441/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0220 12:17:38.627582    1869 api_server.go:103] status: https://127.0.0.1:49441/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0220 12:17:39.107888    1869 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:49441/healthz ...
I0220 12:17:39.128774    1869 api_server.go:279] https://127.0.0.1:49441/healthz returned 200:
ok
I0220 12:17:39.184984    1869 api_server.go:141] control plane version: v1.31.0
I0220 12:17:39.185073    1869 api_server.go:131] duration metric: took 10.578274741s to wait for apiserver health ...
I0220 12:17:39.186602    1869 system_pods.go:43] waiting for kube-system pods to appear ...
I0220 12:17:39.280877    1869 system_pods.go:59] 7 kube-system pods found
I0220 12:17:39.280945    1869 system_pods.go:61] "coredns-6f6b679f8f-kc7sp" [289d60f2-5923-427f-b14b-89108a515575] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0220 12:17:39.280954    1869 system_pods.go:61] "etcd-minikube" [085a2273-cfc7-442b-a620-b3749d7a2c04] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0220 12:17:39.280962    1869 system_pods.go:61] "kube-apiserver-minikube" [d7e61d56-8144-4a6f-b06c-f90a89805c97] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0220 12:17:39.280973    1869 system_pods.go:61] "kube-controller-manager-minikube" [5d91ad7b-e641-4e2d-a5e9-955b7e414985] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I0220 12:17:39.280992    1869 system_pods.go:61] "kube-proxy-z8hnv" [b9e43b6d-e55d-464d-9673-096f57b40054] Running / Ready:ContainersNotReady (containers with unready status: [kube-proxy]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-proxy])
I0220 12:17:39.280997    1869 system_pods.go:61] "kube-scheduler-minikube" [54a95006-4844-493a-8599-3871ef367862] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0220 12:17:39.281001    1869 system_pods.go:61] "storage-provisioner" [d11e5e94-55ec-48de-a8cc-cbfd5e97fdbe] Running / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I0220 12:17:39.281008    1869 system_pods.go:74] duration metric: took 94.393734ms to wait for pod list to return data ...
I0220 12:17:39.281018    1869 kubeadm.go:582] duration metric: took 15.32519858s to wait for: map[apiserver:true system_pods:true]
I0220 12:17:39.281033    1869 node_conditions.go:102] verifying NodePressure condition ...
I0220 12:17:39.289915    1869 node_conditions.go:122] node storage ephemeral capacity is 61255492Ki
I0220 12:17:39.289966    1869 node_conditions.go:123] node cpu capacity is 2
I0220 12:17:39.290410    1869 node_conditions.go:105] duration metric: took 9.358545ms to run NodePressure ...
I0220 12:17:39.290424    1869 start.go:241] waiting for startup goroutines ...
I0220 12:17:39.507840    1869 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: (9.415361044s)
I0220 12:17:39.507918    1869 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: (9.122231231s)
I0220 12:17:39.576756    1869 out.go:177] 🌟  Enabled addons: storage-provisioner, default-storageclass
I0220 12:17:39.631806    1869 addons.go:510] duration metric: took 15.678192803s for enable addons: enabled=[storage-provisioner default-storageclass]
I0220 12:17:39.631853    1869 start.go:246] waiting for cluster config update ...
I0220 12:17:39.631910    1869 start.go:255] writing updated cluster config ...
I0220 12:17:39.633197    1869 ssh_runner.go:195] Run: rm -f paused
I0220 12:17:40.542542    1869 start.go:600] kubectl: 1.32.0, cluster: 1.31.0 (minor skew: 1)
I0220 12:17:40.560625    1869 out.go:177] 🏄  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
Feb 20 17:44:12 minikube dockerd[916]: time="2025-02-20T17:44:12.910443247Z" level=info msg="ignoring event" container=c21dd2027abefc6ba690c308f9bf6c3c945514cfa0f7d237d548b022a4400a67 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 22 20:27:21 minikube dockerd[916]: time="2025-02-22T20:27:21.465018288Z" level=info msg="ignoring event" container=bf1adfd837b33582dc15ccad945ec71b82f2f5cf4a57b55bf550f9e914bc0f4e module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 22 20:27:21 minikube dockerd[916]: time="2025-02-22T20:27:21.692494573Z" level=info msg="ignoring event" container=9665c5a8746709baba64f9e4ba3ac40b31c12c41cfa6b461eed383b8aab2a57c module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 22 20:28:35 minikube cri-dockerd[1184]: time="2025-02-22T20:28:35Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/cf6ad861df0a294aa9d4670e2f6a083a49b9a46cf4b5dc54cf229ff73054cb3d/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Feb 22 20:28:45 minikube cri-dockerd[1184]: time="2025-02-22T20:28:45Z" level=info msg="Stop pulling image kodekloud/examplevotingapp_vote:v1: Status: Downloaded newer image for kodekloud/examplevotingapp_vote:v1"
Feb 22 20:41:03 minikube cri-dockerd[1184]: time="2025-02-22T20:41:03Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/7e95734c17d7b0252afe5364eebe2158de951051ee302b0731b45b254de70cd3/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Feb 22 20:41:07 minikube cri-dockerd[1184]: time="2025-02-22T20:41:07Z" level=info msg="Stop pulling image redis:latest: Status: Downloaded newer image for redis:latest"
Feb 22 20:41:50 minikube cri-dockerd[1184]: time="2025-02-22T20:41:50Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/50037b5760f22f62429ae4c89c6be5011c5be9df491b89d00b304a2c23db40fa/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Feb 22 20:42:01 minikube cri-dockerd[1184]: time="2025-02-22T20:42:01Z" level=info msg="Pulling image postgres:latest: 8a3a78e9a28a: Extracting [================>                                  ]  36.77MB/112.8MB"
Feb 22 20:42:11 minikube cri-dockerd[1184]: time="2025-02-22T20:42:11Z" level=info msg="Stop pulling image postgres:latest: Status: Downloaded newer image for postgres:latest"
Feb 22 20:42:11 minikube dockerd[916]: time="2025-02-22T20:42:11.718816080Z" level=info msg="ignoring event" container=9c45344003a56d3284dc48364b24c6b266716df45e435ff7cf3afc350e7bacbd module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 22 20:42:13 minikube cri-dockerd[1184]: time="2025-02-22T20:42:13Z" level=info msg="Stop pulling image postgres:latest: Status: Image is up to date for postgres:latest"
Feb 22 20:42:13 minikube dockerd[916]: time="2025-02-22T20:42:13.494202935Z" level=info msg="ignoring event" container=6393eededa806a5acbba5eef8ab801caf323a4db4f7b12c6fdf03b3ececf9afa module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 22 20:42:29 minikube cri-dockerd[1184]: time="2025-02-22T20:42:29Z" level=info msg="Stop pulling image postgres:latest: Status: Image is up to date for postgres:latest"
Feb 22 20:42:30 minikube dockerd[916]: time="2025-02-22T20:42:30.181777877Z" level=info msg="ignoring event" container=f3e86fb3b8c05bf85288da0f016c16f8c4b26e03e426fc74ea9d601dd8c5cd92 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 22 20:42:56 minikube cri-dockerd[1184]: time="2025-02-22T20:42:56Z" level=info msg="Stop pulling image postgres:latest: Status: Image is up to date for postgres:latest"
Feb 22 20:42:57 minikube dockerd[916]: time="2025-02-22T20:42:57.071106591Z" level=info msg="ignoring event" container=8ee23f7d67037456757bea3c7271bf1c10b7dfd6201dd6cf83a7a7f9640037e8 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 22 20:43:48 minikube cri-dockerd[1184]: time="2025-02-22T20:43:48Z" level=info msg="Stop pulling image postgres:latest: Status: Image is up to date for postgres:latest"
Feb 22 20:43:49 minikube dockerd[916]: time="2025-02-22T20:43:49.107387422Z" level=info msg="ignoring event" container=7a54521cb343bf7bb0e680487e7784c81f0d7c5a78e5ccf8b4778783822433a0 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 22 20:44:52 minikube cri-dockerd[1184]: time="2025-02-22T20:44:52Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/a8417af8f0b68ee8ce52161dd6f4342af580a3d931d07902d79a45d0a7c11072/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Feb 22 20:45:03 minikube cri-dockerd[1184]: time="2025-02-22T20:45:03Z" level=info msg="Pulling image kodekloud/examplevotingapp_worker:v1: 3e17c6eae66c: Extracting [===========================================>       ]  39.45MB/45.13MB"
Feb 22 20:45:13 minikube cri-dockerd[1184]: time="2025-02-22T20:45:13Z" level=info msg="Pulling image kodekloud/examplevotingapp_worker:v1: b800e4c6f9e9: Downloading [================>                                  ]   93.6MB/277MB"
Feb 22 20:45:22 minikube cri-dockerd[1184]: time="2025-02-22T20:45:22Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/978dfc12168b1662cd32dbcfda567738184d055126547b04da12f5feed5b009d/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Feb 22 20:45:23 minikube cri-dockerd[1184]: time="2025-02-22T20:45:23Z" level=info msg="Pulling image kodekloud/examplevotingapp_worker:v1: b800e4c6f9e9: Downloading [====================================>              ]  200.7MB/277MB"
Feb 22 20:45:33 minikube cri-dockerd[1184]: time="2025-02-22T20:45:33Z" level=info msg="Pulling image kodekloud/examplevotingapp_worker:v1: e857bd06f538: Extracting [=>                                                 ]  4.456MB/137.3MB"
Feb 22 20:45:43 minikube cri-dockerd[1184]: time="2025-02-22T20:45:43Z" level=info msg="Pulling image kodekloud/examplevotingapp_worker:v1: e857bd06f538: Extracting [=========================================>         ]  113.1MB/137.3MB"
Feb 22 20:45:54 minikube cri-dockerd[1184]: time="2025-02-22T20:45:54Z" level=info msg="Pulling image kodekloud/examplevotingapp_worker:v1: b800e4c6f9e9: Extracting [=========>                                         ]  55.15MB/277MB"
Feb 22 20:46:03 minikube cri-dockerd[1184]: time="2025-02-22T20:46:03Z" level=info msg="Pulling image kodekloud/examplevotingapp_worker:v1: b800e4c6f9e9: Extracting [====================>                              ]  111.4MB/277MB"
Feb 22 20:46:13 minikube cri-dockerd[1184]: time="2025-02-22T20:46:13Z" level=info msg="Pulling image kodekloud/examplevotingapp_worker:v1: b800e4c6f9e9: Extracting [===========================>                       ]  154.3MB/277MB"
Feb 22 20:46:23 minikube cri-dockerd[1184]: time="2025-02-22T20:46:23Z" level=info msg="Pulling image kodekloud/examplevotingapp_worker:v1: b800e4c6f9e9: Extracting [====================================>              ]  200.5MB/277MB"
Feb 22 20:46:33 minikube cri-dockerd[1184]: time="2025-02-22T20:46:33Z" level=info msg="Pulling image kodekloud/examplevotingapp_worker:v1: b800e4c6f9e9: Extracting [=============================================>     ]  254.6MB/277MB"
Feb 22 20:46:42 minikube cri-dockerd[1184]: time="2025-02-22T20:46:42Z" level=info msg="Stop pulling image kodekloud/examplevotingapp_worker:v1: Status: Downloaded newer image for kodekloud/examplevotingapp_worker:v1"
Feb 22 20:46:44 minikube cri-dockerd[1184]: time="2025-02-22T20:46:44Z" level=info msg="Stop pulling image postgres:latest: Status: Image is up to date for postgres:latest"
Feb 22 20:46:44 minikube dockerd[916]: time="2025-02-22T20:46:44.919598107Z" level=info msg="ignoring event" container=e9259bbf66a17a049a418feccbcfe1bc5b293205e24e8c86ea5f4166c9a0eda4 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 22 20:46:50 minikube cri-dockerd[1184]: time="2025-02-22T20:46:50Z" level=error msg="error getting RW layer size for container ID '7a54521cb343bf7bb0e680487e7784c81f0d7c5a78e5ccf8b4778783822433a0': Error response from daemon: No such container: 7a54521cb343bf7bb0e680487e7784c81f0d7c5a78e5ccf8b4778783822433a0"
Feb 22 20:46:50 minikube cri-dockerd[1184]: time="2025-02-22T20:46:50Z" level=error msg="Set backoffDuration to : 1m0s for container ID '7a54521cb343bf7bb0e680487e7784c81f0d7c5a78e5ccf8b4778783822433a0'"
Feb 22 20:46:55 minikube cri-dockerd[1184]: time="2025-02-22T20:46:55Z" level=info msg="Pulling image kodekloud/examplevotingapp_result:v1: aa41d054f582: Extracting [===============================================>   ]  42.66MB/45.15MB"
Feb 22 20:46:59 minikube cri-dockerd[1184]: time="2025-02-22T20:46:59Z" level=info msg="Stop pulling image kodekloud/examplevotingapp_result:v1: Status: Downloaded newer image for kodekloud/examplevotingapp_result:v1"
Feb 22 20:48:19 minikube dockerd[916]: time="2025-02-22T20:48:19.925709247Z" level=info msg="ignoring event" container=6aaa758ecdccb3bff615eeecc836f63dfd7488c477e4bd0b7a482ee0308d9bae module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 22 20:48:20 minikube dockerd[916]: time="2025-02-22T20:48:20.079120729Z" level=info msg="ignoring event" container=cf6ad861df0a294aa9d4670e2f6a083a49b9a46cf4b5dc54cf229ff73054cb3d module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 22 20:49:08 minikube dockerd[916]: time="2025-02-22T20:49:08.693538625Z" level=info msg="ignoring event" container=58c20ad388619787c898bbb4a61aada0934cdd1d6054b86374075ed70891df63 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 22 20:49:08 minikube dockerd[916]: time="2025-02-22T20:49:08.823667664Z" level=info msg="ignoring event" container=978dfc12168b1662cd32dbcfda567738184d055126547b04da12f5feed5b009d module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 22 20:49:25 minikube cri-dockerd[1184]: time="2025-02-22T20:49:25Z" level=info msg="Stop pulling image postgres:latest: Status: Image is up to date for postgres:latest"
Feb 22 20:49:26 minikube dockerd[916]: time="2025-02-22T20:49:26.068624150Z" level=info msg="ignoring event" container=9e279ad7563cd6809749a6b170cf1867a215b49b17211a188452f1b4e36d0fea module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 22 20:50:03 minikube cri-dockerd[1184]: time="2025-02-22T20:50:03Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/bfa0e449e33795c79b4fe3ba769e9c31ce7019bcb3b396f0663c0a890119b236/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Feb 22 20:50:34 minikube dockerd[916]: time="2025-02-22T20:50:34.597765023Z" level=info msg="ignoring event" container=50037b5760f22f62429ae4c89c6be5011c5be9df491b89d00b304a2c23db40fa module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 22 20:50:45 minikube cri-dockerd[1184]: time="2025-02-22T20:50:45Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/af11a8ece193938b54b20dae581d38f992afed18834e4353d044606c8065d7f2/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Feb 22 20:50:46 minikube cri-dockerd[1184]: time="2025-02-22T20:50:46Z" level=info msg="Stop pulling image postgres:latest: Status: Image is up to date for postgres:latest"
Feb 22 20:50:46 minikube dockerd[916]: time="2025-02-22T20:50:46.296016565Z" level=info msg="ignoring event" container=4e132b1450258265a55ae3b12ce8016d5a4864c15166604007dc5f50c3ae7478 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 22 20:50:47 minikube cri-dockerd[1184]: time="2025-02-22T20:50:47Z" level=info msg="Stop pulling image postgres:latest: Status: Image is up to date for postgres:latest"
Feb 22 20:50:47 minikube dockerd[916]: time="2025-02-22T20:50:47.356479088Z" level=info msg="ignoring event" container=9c93f17d95e5a38002c259599664fd565a994fe0392b07f426822c281ead874d module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 22 20:51:00 minikube cri-dockerd[1184]: time="2025-02-22T20:51:00Z" level=info msg="Stop pulling image postgres:latest: Status: Image is up to date for postgres:latest"
Feb 22 20:51:00 minikube dockerd[916]: time="2025-02-22T20:51:00.856111772Z" level=info msg="ignoring event" container=5dea819f4bb20f75dada7b0b89a7143803da2b3555413b14e85e0369817796a2 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 22 20:51:29 minikube cri-dockerd[1184]: time="2025-02-22T20:51:29Z" level=info msg="Stop pulling image postgres:latest: Status: Image is up to date for postgres:latest"
Feb 22 20:51:29 minikube dockerd[916]: time="2025-02-22T20:51:29.951278736Z" level=info msg="ignoring event" container=d83e8b3e33375f6e2bccccf68ab0081e5b2e15875f6d7c6b9ecf659ae21bd831 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 22 20:52:19 minikube cri-dockerd[1184]: time="2025-02-22T20:52:19Z" level=info msg="Stop pulling image postgres:latest: Status: Image is up to date for postgres:latest"
Feb 22 20:52:19 minikube dockerd[916]: time="2025-02-22T20:52:19.961539645Z" level=info msg="ignoring event" container=d95e8f4a14690f6646904f72989fe201c05a81cd8741143cac64907ab832553f module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 22 20:53:27 minikube cri-dockerd[1184]: time="2025-02-22T20:53:27Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/e0ab6f6c842bc16ae341fe741078485241a7d2d3a6a9cab4ff8e5b9b3eef5139/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Feb 22 20:53:49 minikube cri-dockerd[1184]: time="2025-02-22T20:53:49Z" level=info msg="Stop pulling image postgres:latest: Status: Image is up to date for postgres:latest"
Feb 22 20:53:49 minikube dockerd[916]: time="2025-02-22T20:53:49.820734181Z" level=info msg="ignoring event" container=f87d4c121dedd51e95e3e29f848650ad3e6741b2ecbf167353f7de90ca011be0 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"


==> container status <==
CONTAINER           IMAGE                                                                                                       CREATED              STATE               NAME                      ATTEMPT             POD ID              POD
f87d4c121dedd       postgres@sha256:0ab5f0d74775b687e8cc67c3d55a7c8e2157c40a29171a48aefe6ce1cb3a2c3a                            About a minute ago   Exited              postgres                  5                   af11a8ece1939       postgres-pod
c1b535c5237a3       6ce23a8ce243b                                                                                               About a minute ago   Running             result-app                0                   e0ab6f6c842bc       result-app-pod
3e6ee2a35c953       04e406d349f5f                                                                                               5 minutes ago        Running             voting-app                0                   bfa0e449e3379       voting-app-pod
33949176e8fa9       kodekloud/examplevotingapp_worker@sha256:741e3aaaa812af72ce0c7fc5889ba31c3f90c79e650c2cb31807fffc60622263   8 minutes ago        Running             worker-app                0                   a8417af8f0b68       worker-app-pod
27740d85b15f0       redis@sha256:93a8d83b707d0d6a1b9186edecca2e37f83722ae0e398aee4eea0ff17c2fad0e                               13 minutes ago       Running             redis                     0                   7e95734c17d7b       redis-pod
8c347a69b2d85       6e38f40d628db                                                                                               2 days ago           Running             storage-provisioner       4                   b6103e8321f63       storage-provisioner
9685fe2a482e4       cbb01a7bd410d                                                                                               2 days ago           Running             coredns                   1                   6d198286e2381       coredns-6f6b679f8f-kc7sp
1f09d80026fa3       6e38f40d628db                                                                                               2 days ago           Exited              storage-provisioner       3                   b6103e8321f63       storage-provisioner
8098f4cd90603       ad83b2ca7b09e                                                                                               2 days ago           Running             kube-proxy                1                   f2cb19720d52d       kube-proxy-z8hnv
c79afb36d1da1       1766f54c897f0                                                                                               2 days ago           Running             kube-scheduler            1                   855e955e275d1       kube-scheduler-minikube
57234d3f79c11       045733566833c                                                                                               2 days ago           Running             kube-controller-manager   1                   b941c34e87347       kube-controller-manager-minikube
a034382fce7a0       2e96e5913fc06                                                                                               2 days ago           Running             etcd                      1                   837e857e93a1c       etcd-minikube
135fbef44966b       604f5db92eaa8                                                                                               2 days ago           Running             kube-apiserver            1                   b8bc7ac02d691       kube-apiserver-minikube
08096c802923c       cbb01a7bd410d                                                                                               5 weeks ago          Exited              coredns                   0                   bed7086ca0098       coredns-6f6b679f8f-kc7sp
6746c867c24aa       ad83b2ca7b09e                                                                                               5 weeks ago          Exited              kube-proxy                0                   df46d640fa3cd       kube-proxy-z8hnv
fc63264d44925       1766f54c897f0                                                                                               5 weeks ago          Exited              kube-scheduler            0                   7c2892913e9ac       kube-scheduler-minikube
214b15d6fd40d       045733566833c                                                                                               5 weeks ago          Exited              kube-controller-manager   0                   4ad8d8304cd0d       kube-controller-manager-minikube
fd06917928af7       604f5db92eaa8                                                                                               5 weeks ago          Exited              kube-apiserver            0                   fc53ed7611a91       kube-apiserver-minikube
d4783c0ca3583       2e96e5913fc06                                                                                               5 weeks ago          Exited              etcd                      0                   c56cc4f4164e8       etcd-minikube


==> coredns [08096c802923] <==
.:53
[INFO] plugin/reload: Running configuration SHA512 = 8846d9ca81164c00fa03e78dfcf1a6846552cc49335bc010218794b8cfaf537759aa4b596e7dc20c0f618e8eb07603c0139662b99dfa3de45b176fbe7fb57ce1
CoreDNS-1.11.1
linux/amd64, go1.20.7, ae2bbc2
[INFO] 127.0.0.1:40900 - 19206 "HINFO IN 1678384863056055279.6251648113506075992. udp 57 false 512" NXDOMAIN qr,rd,ra 57 0.115938772s
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.001380479s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 2.501581527s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.239091877s


==> coredns [9685fe2a482e] <==
[INFO] 10.244.0.28:45467 - 40961 "A IN db.default.svc.cluster.local. udp 46 false 512" NOERROR qr,aa,rd 90 0.000592401s
[INFO] 10.244.0.28:45467 - 21726 "AAAA IN db.default.svc.cluster.local. udp 46 false 512" NOERROR qr,aa,rd 139 0.001017355s
[INFO] 10.244.0.32:47048 - 53250 "AAAA IN db.default.svc.cluster.local. udp 46 false 512" NOERROR qr,aa,rd 139 0.000464215s
[INFO] 10.244.0.32:47048 - 15630 "A IN db.default.svc.cluster.local. udp 46 false 512" NOERROR qr,aa,rd 90 0.00061352s
[INFO] 10.244.0.28:49501 - 62091 "AAAA IN db.default.svc.cluster.local. udp 46 false 512" NOERROR qr,aa,rd 139 0.00254348s
[INFO] 10.244.0.28:49501 - 49933 "A IN db.default.svc.cluster.local. udp 46 false 512" NOERROR qr,aa,rd 90 0.00204415s
[INFO] 10.244.0.32:56887 - 28439 "AAAA IN db.default.svc.cluster.local. udp 46 false 512" NOERROR qr,aa,rd 139 0.000412173s
[INFO] 10.244.0.32:56887 - 16137 "A IN db.default.svc.cluster.local. udp 46 false 512" NOERROR qr,aa,rd 90 0.001818123s
[INFO] 10.244.0.28:48948 - 23729 "AAAA IN db.default.svc.cluster.local. udp 46 false 512" NOERROR qr,aa,rd 139 0.000240345s
[INFO] 10.244.0.28:48948 - 11328 "A IN db.default.svc.cluster.local. udp 46 false 512" NOERROR qr,aa,rd 90 0.000571058s
[INFO] 10.244.0.32:51634 - 17388 "AAAA IN db.default.svc.cluster.local. udp 46 false 512" NOERROR qr,aa,rd 139 0.000341754s
[INFO] 10.244.0.32:51634 - 24038 "A IN db.default.svc.cluster.local. udp 46 false 512" NOERROR qr,aa,rd 90 0.000675313s
[INFO] 10.244.0.28:40575 - 16171 "AAAA IN db.default.svc.cluster.local. udp 46 false 512" NOERROR qr,aa,rd 139 0.000175042s
[INFO] 10.244.0.28:40575 - 32212 "A IN db.default.svc.cluster.local. udp 46 false 512" NOERROR qr,aa,rd 90 0.000116797s
[INFO] 10.244.0.32:34238 - 64776 "A IN db.default.svc.cluster.local. udp 46 false 512" NOERROR qr,aa,rd 90 0.000209726s
[INFO] 10.244.0.32:34238 - 62220 "AAAA IN db.default.svc.cluster.local. udp 46 false 512" NOERROR qr,aa,rd 139 0.000419907s
[INFO] 10.244.0.28:44646 - 39486 "AAAA IN db.default.svc.cluster.local. udp 46 false 512" NOERROR qr,aa,rd 139 0.00038384s
[INFO] 10.244.0.28:44646 - 59759 "A IN db.default.svc.cluster.local. udp 46 false 512" NOERROR qr,aa,rd 90 0.000196638s
[INFO] 10.244.0.32:54355 - 22948 "A IN db.default.svc.cluster.local. udp 46 false 512" NOERROR qr,aa,rd 90 0.000168241s
[INFO] 10.244.0.32:54355 - 15278 "AAAA IN db.default.svc.cluster.local. udp 46 false 512" NOERROR qr,aa,rd 139 0.000779316s
[INFO] 10.244.0.28:43006 - 11532 "A IN db.default.svc.cluster.local. udp 46 false 512" NOERROR qr,aa,rd 90 0.000669872s
[INFO] 10.244.0.28:43006 - 41449 "AAAA IN db.default.svc.cluster.local. udp 46 false 512" NOERROR qr,aa,rd 139 0.000826732s
[INFO] 10.244.0.32:55138 - 43163 "A IN db.default.svc.cluster.local. udp 46 false 512" NOERROR qr,aa,rd 90 0.000360437s
[INFO] 10.244.0.32:55138 - 61588 "AAAA IN db.default.svc.cluster.local. udp 46 false 512" NOERROR qr,aa,rd 139 0.00144303s
[INFO] 10.244.0.28:44178 - 18056 "A IN db.default.svc.cluster.local. udp 46 false 512" NOERROR qr,aa,rd 90 0.001091033s
[INFO] 10.244.0.28:44178 - 59123 "AAAA IN db.default.svc.cluster.local. udp 46 false 512" NOERROR qr,aa,rd 139 0.0011386s
[INFO] 10.244.0.32:51908 - 3213 "AAAA IN db.default.svc.cluster.local. udp 46 false 512" NOERROR qr,aa,rd 139 0.000096115s
[INFO] 10.244.0.32:51908 - 28978 "A IN db.default.svc.cluster.local. udp 46 false 512" NOERROR qr,aa,rd 90 0.000056492s
[INFO] 10.244.0.28:44816 - 57649 "AAAA IN db.default.svc.cluster.local. udp 46 false 512" NOERROR qr,aa,rd 139 0.000350727s
[INFO] 10.244.0.28:44816 - 31622 "A IN db.default.svc.cluster.local. udp 46 false 512" NOERROR qr,aa,rd 90 0.000683154s
[INFO] 10.244.0.32:53960 - 48531 "AAAA IN db.default.svc.cluster.local. udp 46 false 512" NOERROR qr,aa,rd 139 0.000279472s
[INFO] 10.244.0.32:53960 - 49545 "A IN db.default.svc.cluster.local. udp 46 false 512" NOERROR qr,aa,rd 90 0.000147418s
[INFO] 10.244.0.28:41900 - 3807 "AAAA IN db.default.svc.cluster.local. udp 46 false 512" NOERROR qr,aa,rd 139 0.000451497s
[INFO] 10.244.0.28:41900 - 2657 "A IN db.default.svc.cluster.local. udp 46 false 512" NOERROR qr,aa,rd 90 0.000801783s
[INFO] 10.244.0.32:51879 - 42862 "A IN db.default.svc.cluster.local. udp 46 false 512" NOERROR qr,aa,rd 90 0.00027968s
[INFO] 10.244.0.32:51879 - 16720 "AAAA IN db.default.svc.cluster.local. udp 46 false 512" NOERROR qr,aa,rd 139 0.000551751s
[INFO] 10.244.0.28:51176 - 46565 "AAAA IN db.default.svc.cluster.local. udp 46 false 512" NOERROR qr,aa,rd 139 0.000181725s
[INFO] 10.244.0.28:51176 - 20483 "A IN db.default.svc.cluster.local. udp 46 false 512" NOERROR qr,aa,rd 90 0.000931291s
[INFO] 10.244.0.32:33687 - 56739 "AAAA IN db.default.svc.cluster.local. udp 46 false 512" NOERROR qr,aa,rd 139 0.000582049s
[INFO] 10.244.0.32:33687 - 35289 "A IN db.default.svc.cluster.local. udp 46 false 512" NOERROR qr,aa,rd 90 0.000581443s
[INFO] 10.244.0.28:39489 - 2134 "AAAA IN db.default.svc.cluster.local. udp 46 false 512" NOERROR qr,aa,rd 139 0.00024141s
[INFO] 10.244.0.28:39489 - 60946 "A IN db.default.svc.cluster.local. udp 46 false 512" NOERROR qr,aa,rd 90 0.000169464s
[INFO] 10.244.0.32:57858 - 56414 "AAAA IN db.default.svc.cluster.local. udp 46 false 512" NOERROR qr,aa,rd 139 0.000249167s
[INFO] 10.244.0.32:57858 - 55124 "A IN db.default.svc.cluster.local. udp 46 false 512" NOERROR qr,aa,rd 90 0.000995598s
[INFO] 10.244.0.28:43302 - 55198 "AAAA IN db.default.svc.cluster.local. udp 46 false 512" NOERROR qr,aa,rd 139 0.000328367s
[INFO] 10.244.0.28:43302 - 6330 "A IN db.default.svc.cluster.local. udp 46 false 512" NOERROR qr,aa,rd 90 0.001115362s
[INFO] 10.244.0.32:37796 - 32122 "AAAA IN db.default.svc.cluster.local. udp 46 false 512" NOERROR qr,aa,rd 139 0.000088875s
[INFO] 10.244.0.32:37796 - 38783 "A IN db.default.svc.cluster.local. udp 46 false 512" NOERROR qr,aa,rd 90 0.00004959s
[INFO] 10.244.0.28:37134 - 46593 "A IN db.default.svc.cluster.local. udp 46 false 512" NOERROR qr,aa,rd 90 0.000260597s
[INFO] 10.244.0.28:37134 - 35765 "AAAA IN db.default.svc.cluster.local. udp 46 false 512" NOERROR qr,aa,rd 139 0.000356086s
[INFO] 10.244.0.32:40345 - 35880 "A IN db.default.svc.cluster.local. udp 46 false 512" NOERROR qr,aa,rd 90 0.00009253s
[INFO] 10.244.0.32:40345 - 2580 "AAAA IN db.default.svc.cluster.local. udp 46 false 512" NOERROR qr,aa,rd 139 0.000053192s
[INFO] 10.244.0.28:35085 - 59135 "AAAA IN db.default.svc.cluster.local. udp 46 false 512" NOERROR qr,aa,rd 139 0.000118485s
[INFO] 10.244.0.28:35085 - 34634 "A IN db.default.svc.cluster.local. udp 46 false 512" NOERROR qr,aa,rd 90 0.000144758s
[INFO] 10.244.0.32:38413 - 46926 "AAAA IN db.default.svc.cluster.local. udp 46 false 512" NOERROR qr,aa,rd 139 0.00007619s
[INFO] 10.244.0.32:38413 - 11858 "A IN db.default.svc.cluster.local. udp 46 false 512" NOERROR qr,aa,rd 90 0.000035626s
[INFO] 10.244.0.28:39263 - 1376 "AAAA IN db.default.svc.cluster.local. udp 46 false 512" NOERROR qr,aa,rd 139 0.000190876s
[INFO] 10.244.0.28:39263 - 18693 "A IN db.default.svc.cluster.local. udp 46 false 512" NOERROR qr,aa,rd 90 0.000068237s
[INFO] 10.244.0.32:33677 - 34956 "AAAA IN db.default.svc.cluster.local. udp 46 false 512" NOERROR qr,aa,rd 139 0.000189849s
[INFO] 10.244.0.32:33677 - 16791 "A IN db.default.svc.cluster.local. udp 46 false 512" NOERROR qr,aa,rd 90 0.00005638s


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=210b148df93a80eb872ecbeb7e35281b3c582c61
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2025_01_14T15_33_52_0700
                    minikube.k8s.io/version=v1.34.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Tue, 14 Jan 2025 20:33:47 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Sat, 22 Feb 2025 20:55:02 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Sat, 22 Feb 2025 20:52:19 +0000   Thu, 20 Feb 2025 18:47:11 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Sat, 22 Feb 2025 20:52:19 +0000   Thu, 20 Feb 2025 18:47:11 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Sat, 22 Feb 2025 20:52:19 +0000   Thu, 20 Feb 2025 18:47:11 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Sat, 22 Feb 2025 20:52:19 +0000   Thu, 20 Feb 2025 18:47:11 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                2
  ephemeral-storage:  61255492Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             4029192Ki
  pods:               110
Allocatable:
  cpu:                2
  ephemeral-storage:  61255492Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             4029192Ki
  pods:               110
System Info:
  Machine ID:                 612f22dad83446dea04acae2b22971c3
  System UUID:                192ce3b0-4b9d-44be-a3f4-e38b779f913b
  Boot ID:                    b4f73d38-e1ff-4ead-808b-4025a8778d33
  Kernel Version:             5.10.104-linuxkit
  OS Image:                   Ubuntu 22.04.4 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://27.2.0
  Kubelet Version:            v1.31.0
  Kube-Proxy Version:         
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (12 in total)
  Namespace                   Name                                CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                ------------  ----------  ---------------  -------------  ---
  default                     postgres-pod                        0 (0%)        0 (0%)      0 (0%)           0 (0%)         4m20s
  default                     redis-pod                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         14m
  default                     result-app-pod                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         98s
  default                     voting-app-pod                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         5m2s
  default                     worker-app-pod                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         10m
  kube-system                 coredns-6f6b679f8f-kc7sp            100m (5%)     0 (0%)      70Mi (1%)        170Mi (4%)     39d
  kube-system                 etcd-minikube                       100m (5%)     0 (0%)      100Mi (2%)       0 (0%)         39d
  kube-system                 kube-apiserver-minikube             250m (12%)    0 (0%)      0 (0%)           0 (0%)         39d
  kube-system                 kube-controller-manager-minikube    200m (10%)    0 (0%)      0 (0%)           0 (0%)         39d
  kube-system                 kube-proxy-z8hnv                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         39d
  kube-system                 kube-scheduler-minikube             100m (5%)     0 (0%)      0 (0%)           0 (0%)         39d
  kube-system                 storage-provisioner                 0 (0%)        0 (0%)      0 (0%)           0 (0%)         39d
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                750m (37%)  0 (0%)
  memory             170Mi (4%)  170Mi (4%)
  ephemeral-storage  0 (0%)      0 (0%)
  hugepages-1Gi      0 (0%)      0 (0%)
  hugepages-2Mi      0 (0%)      0 (0%)
Events:              <none>


==> dmesg <==
[Feb21 17:04] ERROR: earlyprintk= earlyser already used
[  +0.000000] ERROR: earlyprintk= earlyser already used
[  +0.000000] ACPI BIOS Warning (bug): Incorrect checksum in table [DSDT] - 0x7E, should be 0xDB (20200925/tbprint-173)
[  +5.116587] Hangcheck: starting hangcheck timer 0.9.1 (tick is 180 seconds, margin is 60 seconds).
[  +0.027423] the cryptoloop driver has been deprecated and will be removed in in Linux 5.16
[  +0.056286] ACPI Error: Could not enable RealTimeClock event (20200925/evxfevnt-182)
[  +0.004739] ACPI Warning: Could not enable fixed event - RealTimeClock (4) (20200925/evxface-618)
[  +8.768472] grpcfuse: loading out-of-tree module taints kernel.
[Feb21 17:06] tmpfs: Unknown parameter 'noswap'
[  +6.816661] hrtimer: interrupt took 3594870 ns
[Feb21 18:29] Hangcheck: hangcheck value past margin!
[  +1.157851] systemd-journald[15809]: File /run/log/journal/612f22dad83446dea04acae2b22971c3/system.journal corrupted or uncleanly shut down, renaming and replacing.
[Feb21 19:02] Hangcheck: hangcheck value past margin!
[Feb21 19:20] Hangcheck: hangcheck value past margin!
[Feb21 19:47] Hangcheck: hangcheck value past margin!
[Feb21 20:06] Hangcheck: hangcheck value past margin!
[Feb21 22:10] Hangcheck: hangcheck value past margin!
[Feb22 00:11] Hangcheck: hangcheck value past margin!
[Feb22 00:59] Hangcheck: hangcheck value past margin!
[Feb22 02:59] Hangcheck: hangcheck value past margin!
[Feb22 05:00] Hangcheck: hangcheck value past margin!
[Feb22 07:00] Hangcheck: hangcheck value past margin!
[Feb22 08:12] Hangcheck: hangcheck value past margin!
[Feb22 09:13] Hangcheck: hangcheck value past margin!
[Feb22 14:54] Hangcheck: hangcheck value past margin!
[Feb22 19:25] Hangcheck: hangcheck value past margin!
[Feb22 20:15] Hangcheck: hangcheck value past margin!


==> etcd [a034382fce7a] <==
{"level":"info","ts":"2025-02-22T20:43:43.460231Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":14449,"took":"3.611427ms","hash":2389774782,"current-db-size-bytes":2723840,"current-db-size":"2.7 MB","current-db-size-in-use-bytes":1826816,"current-db-size-in-use":"1.8 MB"}
{"level":"info","ts":"2025-02-22T20:43:43.460558Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2389774782,"revision":14449,"compact-revision":14182}
{"level":"info","ts":"2025-02-22T20:45:11.710998Z","caller":"traceutil/trace.go:171","msg":"trace[984017945] linearizableReadLoop","detail":"{readStateIndex:18208; appliedIndex:18207; }","duration":"133.526677ms","start":"2025-02-22T20:45:11.577353Z","end":"2025-02-22T20:45:11.710880Z","steps":["trace[984017945] 'read index received'  (duration: 133.353111ms)","trace[984017945] 'applied index is now lower than readState.Index'  (duration: 173.009µs)"],"step_count":2}
{"level":"info","ts":"2025-02-22T20:45:11.711194Z","caller":"traceutil/trace.go:171","msg":"trace[225601543] transaction","detail":"{read_only:false; response_revision:14819; number_of_response:1; }","duration":"134.125403ms","start":"2025-02-22T20:45:11.577057Z","end":"2025-02-22T20:45:11.711183Z","steps":["trace[225601543] 'process raft request'  (duration: 133.695994ms)"],"step_count":1}
{"level":"warn","ts":"2025-02-22T20:45:11.728939Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"136.87081ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-02-22T20:45:11.729040Z","caller":"traceutil/trace.go:171","msg":"trace[483336766] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:14819; }","duration":"151.671042ms","start":"2025-02-22T20:45:11.577348Z","end":"2025-02-22T20:45:11.729019Z","steps":["trace[483336766] 'agreement among raft nodes before linearized reading'  (duration: 136.81188ms)"],"step_count":1}
{"level":"info","ts":"2025-02-22T20:45:16.030544Z","caller":"traceutil/trace.go:171","msg":"trace[1184496419] transaction","detail":"{read_only:false; response_revision:14822; number_of_response:1; }","duration":"232.006805ms","start":"2025-02-22T20:45:15.798516Z","end":"2025-02-22T20:45:16.030523Z","steps":["trace[1184496419] 'process raft request'  (duration: 231.893087ms)"],"step_count":1}
{"level":"info","ts":"2025-02-22T20:45:36.632060Z","caller":"traceutil/trace.go:171","msg":"trace[716941873] transaction","detail":"{read_only:false; response_revision:14845; number_of_response:1; }","duration":"144.297685ms","start":"2025-02-22T20:45:36.487732Z","end":"2025-02-22T20:45:36.632030Z","steps":["trace[716941873] 'process raft request'  (duration: 144.101568ms)"],"step_count":1}
{"level":"warn","ts":"2025-02-22T20:45:47.751487Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"224.998256ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-02-22T20:45:47.751544Z","caller":"traceutil/trace.go:171","msg":"trace[707502202] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:14853; }","duration":"225.084595ms","start":"2025-02-22T20:45:47.526446Z","end":"2025-02-22T20:45:47.751531Z","steps":["trace[707502202] 'range keys from in-memory index tree'  (duration: 224.889336ms)"],"step_count":1}
{"level":"info","ts":"2025-02-22T20:45:52.096712Z","caller":"traceutil/trace.go:171","msg":"trace[743713165] transaction","detail":"{read_only:false; response_revision:14857; number_of_response:1; }","duration":"274.02348ms","start":"2025-02-22T20:45:51.822659Z","end":"2025-02-22T20:45:52.096683Z","steps":["trace[743713165] 'process raft request'  (duration: 273.878871ms)"],"step_count":1}
{"level":"warn","ts":"2025-02-22T20:46:03.394172Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"112.912129ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 serializable:true keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-02-22T20:46:03.394237Z","caller":"traceutil/trace.go:171","msg":"trace[30335436] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:14870; }","duration":"113.022176ms","start":"2025-02-22T20:46:03.281200Z","end":"2025-02-22T20:46:03.394222Z","steps":["trace[30335436] 'range keys from in-memory index tree'  (duration: 112.894827ms)"],"step_count":1}
{"level":"warn","ts":"2025-02-22T20:46:03.394479Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"423.900978ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/endpointslices/default/kubernetes\" ","response":"range_response_count:1 size:474"}
{"level":"info","ts":"2025-02-22T20:46:03.394513Z","caller":"traceutil/trace.go:171","msg":"trace[629941210] range","detail":"{range_begin:/registry/endpointslices/default/kubernetes; range_end:; response_count:1; response_revision:14870; }","duration":"423.936554ms","start":"2025-02-22T20:46:02.970566Z","end":"2025-02-22T20:46:03.394503Z","steps":["trace[629941210] 'range keys from in-memory index tree'  (duration: 423.819652ms)"],"step_count":1}
{"level":"warn","ts":"2025-02-22T20:46:03.396584Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-02-22T20:46:02.970528Z","time spent":"424.041524ms","remote":"127.0.0.1:42126","response type":"/etcdserverpb.KV/Range","request count":0,"request size":45,"response count":1,"response size":498,"request content":"key:\"/registry/endpointslices/default/kubernetes\" "}
{"level":"warn","ts":"2025-02-22T20:46:03.403743Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"143.850787ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/flowschemas/\" range_end:\"/registry/flowschemas0\" count_only:true ","response":"range_response_count:0 size:7"}
{"level":"info","ts":"2025-02-22T20:46:03.415408Z","caller":"traceutil/trace.go:171","msg":"trace[1455279432] range","detail":"{range_begin:/registry/flowschemas/; range_end:/registry/flowschemas0; response_count:0; response_revision:14870; }","duration":"143.95984ms","start":"2025-02-22T20:46:03.259868Z","end":"2025-02-22T20:46:03.403828Z","steps":["trace[1455279432] 'count revisions from in-memory index tree'  (duration: 143.778665ms)"],"step_count":1}
{"level":"info","ts":"2025-02-22T20:46:12.689480Z","caller":"traceutil/trace.go:171","msg":"trace[1701180552] transaction","detail":"{read_only:false; response_revision:14877; number_of_response:1; }","duration":"122.995022ms","start":"2025-02-22T20:46:12.566327Z","end":"2025-02-22T20:46:12.689322Z","steps":["trace[1701180552] 'process raft request'  (duration: 122.587994ms)"],"step_count":1}
{"level":"info","ts":"2025-02-22T20:46:16.401457Z","caller":"traceutil/trace.go:171","msg":"trace[1665530176] linearizableReadLoop","detail":"{readStateIndex:18282; appliedIndex:18281; }","duration":"384.16995ms","start":"2025-02-22T20:46:16.017267Z","end":"2025-02-22T20:46:16.401437Z","steps":["trace[1665530176] 'read index received'  (duration: 384.021125ms)","trace[1665530176] 'applied index is now lower than readState.Index'  (duration: 148.072µs)"],"step_count":2}
{"level":"info","ts":"2025-02-22T20:46:16.402213Z","caller":"traceutil/trace.go:171","msg":"trace[816250155] transaction","detail":"{read_only:false; response_revision:14880; number_of_response:1; }","duration":"682.13575ms","start":"2025-02-22T20:46:15.720060Z","end":"2025-02-22T20:46:16.402196Z","steps":["trace[816250155] 'process raft request'  (duration: 681.275332ms)"],"step_count":1}
{"level":"warn","ts":"2025-02-22T20:46:16.405395Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"127.41265ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-02-22T20:46:16.405447Z","caller":"traceutil/trace.go:171","msg":"trace[1246529554] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:14880; }","duration":"127.472607ms","start":"2025-02-22T20:46:16.277960Z","end":"2025-02-22T20:46:16.405433Z","steps":["trace[1246529554] 'agreement among raft nodes before linearized reading'  (duration: 127.389242ms)"],"step_count":1}
{"level":"warn","ts":"2025-02-22T20:46:16.405729Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"388.484445ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/csidrivers/\" range_end:\"/registry/csidrivers0\" count_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-02-22T20:46:16.405809Z","caller":"traceutil/trace.go:171","msg":"trace[1683038341] range","detail":"{range_begin:/registry/csidrivers/; range_end:/registry/csidrivers0; response_count:0; response_revision:14880; }","duration":"388.571453ms","start":"2025-02-22T20:46:16.017226Z","end":"2025-02-22T20:46:16.405798Z","steps":["trace[1683038341] 'agreement among raft nodes before linearized reading'  (duration: 388.459931ms)"],"step_count":1}
{"level":"warn","ts":"2025-02-22T20:46:16.418698Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-02-22T20:46:16.017147Z","time spent":"401.485453ms","remote":"127.0.0.1:42156","response type":"/etcdserverpb.KV/Range","request count":0,"request size":48,"response count":0,"response size":29,"request content":"key:\"/registry/csidrivers/\" range_end:\"/registry/csidrivers0\" count_only:true "}
{"level":"warn","ts":"2025-02-22T20:46:16.419499Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-02-22T20:46:15.720043Z","time spent":"682.252382ms","remote":"127.0.0.1:42100","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":1093,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:14879 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1020 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"info","ts":"2025-02-22T20:46:22.038298Z","caller":"traceutil/trace.go:171","msg":"trace[466669829] transaction","detail":"{read_only:false; response_revision:14883; number_of_response:1; }","duration":"215.471218ms","start":"2025-02-22T20:46:21.822802Z","end":"2025-02-22T20:46:22.038273Z","steps":["trace[466669829] 'process raft request'  (duration: 215.200714ms)"],"step_count":1}
{"level":"info","ts":"2025-02-22T20:46:27.027590Z","caller":"traceutil/trace.go:171","msg":"trace[1801460874] linearizableReadLoop","detail":"{readStateIndex:18292; appliedIndex:18291; }","duration":"199.545982ms","start":"2025-02-22T20:46:26.828032Z","end":"2025-02-22T20:46:27.027578Z","steps":["trace[1801460874] 'read index received'  (duration: 199.409641ms)","trace[1801460874] 'applied index is now lower than readState.Index'  (duration: 135.902µs)"],"step_count":2}
{"level":"warn","ts":"2025-02-22T20:46:27.027684Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"199.635291ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/roles/\" range_end:\"/registry/roles0\" count_only:true ","response":"range_response_count:0 size:7"}
{"level":"info","ts":"2025-02-22T20:46:27.027735Z","caller":"traceutil/trace.go:171","msg":"trace[1627344203] range","detail":"{range_begin:/registry/roles/; range_end:/registry/roles0; response_count:0; response_revision:14888; }","duration":"199.670288ms","start":"2025-02-22T20:46:26.828027Z","end":"2025-02-22T20:46:27.027698Z","steps":["trace[1627344203] 'agreement among raft nodes before linearized reading'  (duration: 199.604746ms)"],"step_count":1}
{"level":"info","ts":"2025-02-22T20:46:27.028006Z","caller":"traceutil/trace.go:171","msg":"trace[1669764228] transaction","detail":"{read_only:false; response_revision:14888; number_of_response:1; }","duration":"512.02075ms","start":"2025-02-22T20:46:26.515979Z","end":"2025-02-22T20:46:27.028000Z","steps":["trace[1669764228] 'process raft request'  (duration: 511.501881ms)"],"step_count":1}
{"level":"warn","ts":"2025-02-22T20:46:27.028057Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-02-22T20:46:26.515963Z","time spent":"512.059024ms","remote":"127.0.0.1:42100","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":1093,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:14887 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1020 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"warn","ts":"2025-02-22T20:46:33.661413Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"402.139425ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 serializable:true keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-02-22T20:46:33.661501Z","caller":"traceutil/trace.go:171","msg":"trace[1288880324] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:14893; }","duration":"402.642003ms","start":"2025-02-22T20:46:33.258845Z","end":"2025-02-22T20:46:33.661487Z","steps":["trace[1288880324] 'range keys from in-memory index tree'  (duration: 402.125672ms)"],"step_count":1}
{"level":"info","ts":"2025-02-22T20:46:33.661633Z","caller":"traceutil/trace.go:171","msg":"trace[1173340811] linearizableReadLoop","detail":"{readStateIndex:18300; appliedIndex:18299; }","duration":"285.934104ms","start":"2025-02-22T20:46:33.375691Z","end":"2025-02-22T20:46:33.661625Z","steps":["trace[1173340811] 'read index received'  (duration: 284.701059ms)","trace[1173340811] 'applied index is now lower than readState.Index'  (duration: 1.23255ms)"],"step_count":2}
{"level":"info","ts":"2025-02-22T20:46:33.661848Z","caller":"traceutil/trace.go:171","msg":"trace[1125832122] transaction","detail":"{read_only:false; response_revision:14894; number_of_response:1; }","duration":"398.543224ms","start":"2025-02-22T20:46:33.263296Z","end":"2025-02-22T20:46:33.661839Z","steps":["trace[1125832122] 'process raft request'  (duration: 397.144286ms)"],"step_count":1}
{"level":"warn","ts":"2025-02-22T20:46:33.661951Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-02-22T20:46:33.263227Z","time spent":"398.678502ms","remote":"127.0.0.1:42122","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":520,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/leases/kube-node-lease/minikube\" mod_revision:14886 > success:<request_put:<key:\"/registry/leases/kube-node-lease/minikube\" value_size:471 >> failure:<request_range:<key:\"/registry/leases/kube-node-lease/minikube\" > >"}
{"level":"warn","ts":"2025-02-22T20:46:33.662128Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"286.49289ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-02-22T20:46:33.664206Z","caller":"traceutil/trace.go:171","msg":"trace[1531687355] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:14894; }","duration":"288.565322ms","start":"2025-02-22T20:46:33.375627Z","end":"2025-02-22T20:46:33.664192Z","steps":["trace[1531687355] 'agreement among raft nodes before linearized reading'  (duration: 286.478179ms)"],"step_count":1}
{"level":"warn","ts":"2025-02-22T20:46:33.663578Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"209.06023ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-02-22T20:46:33.665438Z","caller":"traceutil/trace.go:171","msg":"trace[2012485474] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:14894; }","duration":"210.922547ms","start":"2025-02-22T20:46:33.454498Z","end":"2025-02-22T20:46:33.665420Z","steps":["trace[2012485474] 'agreement among raft nodes before linearized reading'  (duration: 209.032424ms)"],"step_count":1}
{"level":"info","ts":"2025-02-22T20:46:39.362284Z","caller":"traceutil/trace.go:171","msg":"trace[97133933] linearizableReadLoop","detail":"{readStateIndex:18304; appliedIndex:18304; }","duration":"102.746382ms","start":"2025-02-22T20:46:39.259518Z","end":"2025-02-22T20:46:39.362265Z","steps":["trace[97133933] 'read index received'  (duration: 102.740416ms)","trace[97133933] 'applied index is now lower than readState.Index'  (duration: 4.885µs)"],"step_count":2}
{"level":"warn","ts":"2025-02-22T20:46:39.362885Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"103.358415ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-02-22T20:46:39.363266Z","caller":"traceutil/trace.go:171","msg":"trace[2036927347] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:14897; }","duration":"103.739598ms","start":"2025-02-22T20:46:39.259511Z","end":"2025-02-22T20:46:39.363251Z","steps":["trace[2036927347] 'agreement among raft nodes before linearized reading'  (duration: 103.348915ms)"],"step_count":1}
{"level":"info","ts":"2025-02-22T20:46:39.359816Z","caller":"traceutil/trace.go:171","msg":"trace[922265005] transaction","detail":"{read_only:false; response_revision:14897; number_of_response:1; }","duration":"190.257995ms","start":"2025-02-22T20:46:39.169539Z","end":"2025-02-22T20:46:39.359797Z","steps":["trace[922265005] 'process raft request'  (duration: 190.1373ms)"],"step_count":1}
{"level":"warn","ts":"2025-02-22T20:46:43.707135Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"321.420558ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" ","response":"range_response_count:1 size:1109"}
{"level":"info","ts":"2025-02-22T20:46:43.711646Z","caller":"traceutil/trace.go:171","msg":"trace[657876523] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:14901; }","duration":"325.903011ms","start":"2025-02-22T20:46:43.385651Z","end":"2025-02-22T20:46:43.711554Z","steps":["trace[657876523] 'range keys from in-memory index tree'  (duration: 321.204462ms)"],"step_count":1}
{"level":"warn","ts":"2025-02-22T20:46:43.711745Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"447.23997ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 serializable:true keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"warn","ts":"2025-02-22T20:46:43.711772Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-02-22T20:46:43.385525Z","time spent":"326.197827ms","remote":"127.0.0.1:42100","response type":"/etcdserverpb.KV/Range","request count":0,"request size":67,"response count":1,"response size":1133,"request content":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" "}
{"level":"info","ts":"2025-02-22T20:46:43.712329Z","caller":"traceutil/trace.go:171","msg":"trace[717665891] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:14901; }","duration":"453.156119ms","start":"2025-02-22T20:46:43.258678Z","end":"2025-02-22T20:46:43.711834Z","steps":["trace[717665891] 'range keys from in-memory index tree'  (duration: 447.198236ms)"],"step_count":1}
{"level":"warn","ts":"2025-02-22T20:46:43.712604Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"255.35176ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-02-22T20:46:43.712660Z","caller":"traceutil/trace.go:171","msg":"trace[1360120192] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:14901; }","duration":"255.425629ms","start":"2025-02-22T20:46:43.457225Z","end":"2025-02-22T20:46:43.712651Z","steps":["trace[1360120192] 'range keys from in-memory index tree'  (duration: 255.160931ms)"],"step_count":1}
{"level":"info","ts":"2025-02-22T20:46:52.085599Z","caller":"traceutil/trace.go:171","msg":"trace[1321283856] transaction","detail":"{read_only:false; response_revision:14911; number_of_response:1; }","duration":"103.694726ms","start":"2025-02-22T20:46:51.981889Z","end":"2025-02-22T20:46:52.085583Z","steps":["trace[1321283856] 'process raft request'  (duration: 103.607434ms)"],"step_count":1}
{"level":"info","ts":"2025-02-22T20:48:43.257251Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":14742}
{"level":"info","ts":"2025-02-22T20:48:43.262416Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":14742,"took":"3.947417ms","hash":1029189601,"current-db-size-bytes":3235840,"current-db-size":"3.2 MB","current-db-size-in-use-bytes":2265088,"current-db-size-in-use":"2.3 MB"}
{"level":"info","ts":"2025-02-22T20:48:43.262640Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1029189601,"revision":14742,"compact-revision":14449}
{"level":"info","ts":"2025-02-22T20:53:43.059080Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":15017}
{"level":"info","ts":"2025-02-22T20:53:43.068422Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":15017,"took":"8.724667ms","hash":3741219127,"current-db-size-bytes":3366912,"current-db-size":"3.4 MB","current-db-size-in-use-bytes":2424832,"current-db-size-in-use":"2.4 MB"}
{"level":"info","ts":"2025-02-22T20:53:43.068539Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3741219127,"revision":15017,"compact-revision":14742}


==> etcd [d4783c0ca358] <==
{"level":"warn","ts":"2025-01-16T05:21:36.441259Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"274.692213ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/serviceaccounts/kube-system/generic-garbage-collector\" ","response":"range_response_count:1 size:216"}
{"level":"info","ts":"2025-01-16T05:21:36.441329Z","caller":"traceutil/trace.go:171","msg":"trace[22945568] range","detail":"{range_begin:/registry/serviceaccounts/kube-system/generic-garbage-collector; range_end:; response_count:1; response_revision:9674; }","duration":"274.766033ms","start":"2025-01-16T05:21:36.166547Z","end":"2025-01-16T05:21:36.441313Z","steps":["trace[22945568] 'agreement among raft nodes before linearized reading'  (duration: 252.28268ms)","trace[22945568] 'range keys from in-memory index tree'  (duration: 22.39076ms)"],"step_count":2}
{"level":"info","ts":"2025-01-16T05:21:36.441785Z","caller":"traceutil/trace.go:171","msg":"trace[1115766352] transaction","detail":"{read_only:false; response_revision:9676; number_of_response:1; }","duration":"176.633126ms","start":"2025-01-16T05:21:36.265138Z","end":"2025-01-16T05:21:36.441771Z","steps":["trace[1115766352] 'process raft request'  (duration: 176.592754ms)"],"step_count":1}
{"level":"info","ts":"2025-01-16T05:21:36.443466Z","caller":"traceutil/trace.go:171","msg":"trace[119640047] transaction","detail":"{read_only:false; response_revision:9675; number_of_response:1; }","duration":"229.501937ms","start":"2025-01-16T05:21:36.213943Z","end":"2025-01-16T05:21:36.443445Z","steps":["trace[119640047] 'process raft request'  (duration: 220.902897ms)"],"step_count":1}
{"level":"info","ts":"2025-01-16T05:21:36.444185Z","caller":"traceutil/trace.go:171","msg":"trace[1845059866] linearizableReadLoop","detail":"{readStateIndex:11909; appliedIndex:11908; }","duration":"184.88517ms","start":"2025-01-16T05:21:36.259279Z","end":"2025-01-16T05:21:36.444164Z","steps":["trace[1845059866] 'read index received'  (duration: 172.66762ms)","trace[1845059866] 'applied index is now lower than readState.Index'  (duration: 12.2159ms)"],"step_count":2}
{"level":"warn","ts":"2025-01-16T05:21:36.445116Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"231.230772ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/ranges/serviceips\" ","response":"range_response_count:1 size:116"}
{"level":"info","ts":"2025-01-16T05:21:36.451813Z","caller":"traceutil/trace.go:171","msg":"trace[422249677] range","detail":"{range_begin:/registry/ranges/serviceips; range_end:; response_count:1; response_revision:9676; }","duration":"237.837075ms","start":"2025-01-16T05:21:36.213866Z","end":"2025-01-16T05:21:36.451703Z","steps":["trace[422249677] 'agreement among raft nodes before linearized reading'  (duration: 231.165684ms)"],"step_count":1}
{"level":"info","ts":"2025-01-16T05:21:36.459020Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":9599}
{"level":"info","ts":"2025-01-16T05:21:36.461329Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":9599,"took":"1.964994ms","hash":693168336,"current-db-size-bytes":1777664,"current-db-size":"1.8 MB","current-db-size-in-use-bytes":647168,"current-db-size-in-use":"647 kB"}
{"level":"info","ts":"2025-01-16T05:21:36.461381Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":693168336,"revision":9599,"compact-revision":9581}
{"level":"warn","ts":"2025-01-16T07:22:21.204308Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"195.441257ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128034595546871387 > lease_revoke:<id:70cc946685a8354a>","response":"size:29"}
{"level":"warn","ts":"2025-01-16T07:22:21.398352Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"193.926869ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128034595546871388 > lease_revoke:<id:70cc946685a83618>","response":"size:29"}
{"level":"warn","ts":"2025-01-16T07:22:21.990839Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"282.667263ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/volumeattachments/\" range_end:\"/registry/volumeattachments0\" count_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-01-16T07:22:21.991326Z","caller":"traceutil/trace.go:171","msg":"trace[527819187] range","detail":"{range_begin:/registry/volumeattachments/; range_end:/registry/volumeattachments0; response_count:0; response_revision:9743; }","duration":"283.19036ms","start":"2025-01-16T07:22:21.708054Z","end":"2025-01-16T07:22:21.991244Z","steps":["trace[527819187] 'get authentication metadata'  (duration: 272.4025ms)"],"step_count":1}
{"level":"warn","ts":"2025-01-16T07:22:21.998134Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-01-16T07:22:21.695881Z","time spent":"302.216882ms","remote":"127.0.0.1:47482","response type":"/etcdserverpb.KV/Range","request count":0,"request size":64,"response count":3,"response size":31,"request content":"key:\"/registry/services/endpoints/\" range_end:\"/registry/services/endpoints0\" count_only:true "}
{"level":"info","ts":"2025-01-16T07:22:22.252300Z","caller":"traceutil/trace.go:171","msg":"trace[391974813] linearizableReadLoop","detail":"{readStateIndex:11990; appliedIndex:11990; }","duration":"130.724696ms","start":"2025-01-16T07:22:22.121561Z","end":"2025-01-16T07:22:22.252286Z","steps":["trace[391974813] 'read index received'  (duration: 130.719044ms)","trace[391974813] 'applied index is now lower than readState.Index'  (duration: 4.947µs)"],"step_count":2}
{"level":"warn","ts":"2025-01-16T07:22:22.255002Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"133.423858ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-01-16T07:22:22.255611Z","caller":"traceutil/trace.go:171","msg":"trace[695172975] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:9743; }","duration":"134.036631ms","start":"2025-01-16T07:22:22.121555Z","end":"2025-01-16T07:22:22.255592Z","steps":["trace[695172975] 'agreement among raft nodes before linearized reading'  (duration: 133.393084ms)"],"step_count":1}
{"level":"info","ts":"2025-01-16T07:22:22.570425Z","caller":"traceutil/trace.go:171","msg":"trace[171787213] linearizableReadLoop","detail":"{readStateIndex:11995; appliedIndex:11995; }","duration":"207.964603ms","start":"2025-01-16T07:22:22.362439Z","end":"2025-01-16T07:22:22.570403Z","steps":["trace[171787213] 'read index received'  (duration: 207.9571ms)","trace[171787213] 'applied index is now lower than readState.Index'  (duration: 6.271µs)"],"step_count":2}
{"level":"warn","ts":"2025-01-16T07:22:22.614376Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"252.20731ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/events/default/minikube.181aadefe950b4b5\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-01-16T07:22:22.614442Z","caller":"traceutil/trace.go:171","msg":"trace[1630819380] range","detail":"{range_begin:/registry/events/default/minikube.181aadefe950b4b5; range_end:; response_count:0; response_revision:9747; }","duration":"252.290932ms","start":"2025-01-16T07:22:22.362135Z","end":"2025-01-16T07:22:22.614426Z","steps":["trace[1630819380] 'agreement among raft nodes before linearized reading'  (duration: 211.0551ms)","trace[1630819380] 'range keys from in-memory index tree'  (duration: 41.125566ms)"],"step_count":2}
{"level":"info","ts":"2025-01-16T07:22:22.828823Z","caller":"traceutil/trace.go:171","msg":"trace[105921686] linearizableReadLoop","detail":"{readStateIndex:11998; appliedIndex:11997; }","duration":"200.594498ms","start":"2025-01-16T07:22:22.628212Z","end":"2025-01-16T07:22:22.828806Z","steps":["trace[105921686] 'read index received'  (duration: 182.462151ms)","trace[105921686] 'applied index is now lower than readState.Index'  (duration: 18.131574ms)"],"step_count":2}
{"level":"info","ts":"2025-01-16T07:22:22.829650Z","caller":"traceutil/trace.go:171","msg":"trace[990993183] transaction","detail":"{read_only:false; response_revision:9750; number_of_response:1; }","duration":"204.160249ms","start":"2025-01-16T07:22:22.625471Z","end":"2025-01-16T07:22:22.829631Z","steps":["trace[990993183] 'process raft request'  (duration: 185.257303ms)","trace[990993183] 'compare'  (duration: 17.99642ms)"],"step_count":2}
{"level":"warn","ts":"2025-01-16T07:22:22.829879Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"204.436163ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/pods/kube-system/coredns-6f6b679f8f-kc7sp\" ","response":"range_response_count:1 size:4908"}
{"level":"info","ts":"2025-01-16T07:22:22.829916Z","caller":"traceutil/trace.go:171","msg":"trace[928936575] range","detail":"{range_begin:/registry/pods/kube-system/coredns-6f6b679f8f-kc7sp; range_end:; response_count:1; response_revision:9750; }","duration":"204.482245ms","start":"2025-01-16T07:22:22.625424Z","end":"2025-01-16T07:22:22.829906Z","steps":["trace[928936575] 'agreement among raft nodes before linearized reading'  (duration: 204.397622ms)"],"step_count":1}
{"level":"warn","ts":"2025-01-16T07:22:22.830131Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"204.991558ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/pods/kube-system/storage-provisioner\" ","response":"range_response_count:1 size:4232"}
{"level":"info","ts":"2025-01-16T07:22:22.830163Z","caller":"traceutil/trace.go:171","msg":"trace[2059328758] range","detail":"{range_begin:/registry/pods/kube-system/storage-provisioner; range_end:; response_count:1; response_revision:9750; }","duration":"205.026596ms","start":"2025-01-16T07:22:22.625128Z","end":"2025-01-16T07:22:22.830154Z","steps":["trace[2059328758] 'agreement among raft nodes before linearized reading'  (duration: 204.961724ms)"],"step_count":1}
{"level":"warn","ts":"2025-01-16T07:22:22.830314Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"205.671232ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/serviceaccounts/kube-system/node-controller\" ","response":"range_response_count:1 size:195"}
{"level":"info","ts":"2025-01-16T07:22:22.830343Z","caller":"traceutil/trace.go:171","msg":"trace[131222088] range","detail":"{range_begin:/registry/serviceaccounts/kube-system/node-controller; range_end:; response_count:1; response_revision:9750; }","duration":"205.700418ms","start":"2025-01-16T07:22:22.624633Z","end":"2025-01-16T07:22:22.830334Z","steps":["trace[131222088] 'agreement among raft nodes before linearized reading'  (duration: 205.646952ms)"],"step_count":1}
{"level":"warn","ts":"2025-01-16T07:22:22.830541Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"205.942158ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/serviceaccounts/kube-system/resourcequota-controller\" ","response":"range_response_count:1 size:214"}
{"level":"info","ts":"2025-01-16T07:22:22.830574Z","caller":"traceutil/trace.go:171","msg":"trace[864733058] range","detail":"{range_begin:/registry/serviceaccounts/kube-system/resourcequota-controller; range_end:; response_count:1; response_revision:9750; }","duration":"205.974279ms","start":"2025-01-16T07:22:22.624588Z","end":"2025-01-16T07:22:22.830563Z","steps":["trace[864733058] 'agreement among raft nodes before linearized reading'  (duration: 205.880133ms)"],"step_count":1}
{"level":"warn","ts":"2025-01-16T07:22:22.832453Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"207.965774ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/ranges/serviceips\" ","response":"range_response_count:1 size:116"}
{"level":"info","ts":"2025-01-16T07:22:22.832556Z","caller":"traceutil/trace.go:171","msg":"trace[728692083] range","detail":"{range_begin:/registry/ranges/serviceips; range_end:; response_count:1; response_revision:9750; }","duration":"208.074044ms","start":"2025-01-16T07:22:22.624466Z","end":"2025-01-16T07:22:22.832540Z","steps":["trace[728692083] 'agreement among raft nodes before linearized reading'  (duration: 207.88043ms)"],"step_count":1}
{"level":"warn","ts":"2025-01-16T07:22:22.832862Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"216.309438ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/pods/default/myapp-deployment-5684855748-ltwlp\" ","response":"range_response_count:1 size:2836"}
{"level":"info","ts":"2025-01-16T07:22:22.832940Z","caller":"traceutil/trace.go:171","msg":"trace[750839040] range","detail":"{range_begin:/registry/pods/default/myapp-deployment-5684855748-ltwlp; range_end:; response_count:1; response_revision:9750; }","duration":"216.390418ms","start":"2025-01-16T07:22:22.616538Z","end":"2025-01-16T07:22:22.832928Z","steps":["trace[750839040] 'agreement among raft nodes before linearized reading'  (duration: 216.238891ms)"],"step_count":1}
{"level":"warn","ts":"2025-01-16T07:22:22.833155Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"216.64033ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" ","response":"range_response_count:1 size:688"}
{"level":"info","ts":"2025-01-16T07:22:22.833224Z","caller":"traceutil/trace.go:171","msg":"trace[842601602] range","detail":"{range_begin:/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii; range_end:; response_count:1; response_revision:9750; }","duration":"216.711654ms","start":"2025-01-16T07:22:22.616503Z","end":"2025-01-16T07:22:22.833214Z","steps":["trace[842601602] 'agreement among raft nodes before linearized reading'  (duration: 216.580158ms)"],"step_count":1}
{"level":"info","ts":"2025-01-16T07:22:22.885293Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":9676}
{"level":"info","ts":"2025-01-16T07:22:22.900180Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":9676,"took":"13.804676ms","hash":747124141,"current-db-size-bytes":1777664,"current-db-size":"1.8 MB","current-db-size-in-use-bytes":688128,"current-db-size-in-use":"688 kB"}
{"level":"info","ts":"2025-01-16T07:22:22.900272Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":747124141,"revision":9676,"compact-revision":9599}
{"level":"warn","ts":"2025-01-16T07:22:33.560197Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"123.029911ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128034595546871507 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/events/default/minikube.181aac5be458e31d\" mod_revision:0 > success:<request_put:<key:\"/registry/events/default/minikube.181aac5be458e31d\" value_size:546 lease:8128034595546871430 >> failure:<>>","response":"size:16"}
{"level":"info","ts":"2025-01-16T07:22:33.566411Z","caller":"traceutil/trace.go:171","msg":"trace[571862573] transaction","detail":"{read_only:false; response_revision:9770; number_of_response:1; }","duration":"174.866499ms","start":"2025-01-16T07:22:33.390732Z","end":"2025-01-16T07:22:33.565599Z","steps":["trace[571862573] 'process raft request'  (duration: 21.037518ms)","trace[571862573] 'check requests'  (duration: 26.885866ms)","trace[571862573] 'marshal mvccpb.KeyValue' {req_type:put; key:/registry/events/default/minikube.181aac5be458e31d; req_size:611; } (duration: 95.870589ms)"],"step_count":3}
{"level":"info","ts":"2025-01-16T07:22:33.625635Z","caller":"traceutil/trace.go:171","msg":"trace[493805543] linearizableReadLoop","detail":"{readStateIndex:12022; appliedIndex:12020; }","duration":"163.051199ms","start":"2025-01-16T07:22:33.462483Z","end":"2025-01-16T07:22:33.625534Z","steps":["trace[493805543] 'read index received'  (duration: 27.199925ms)","trace[493805543] 'applied index is now lower than readState.Index'  (duration: 135.831603ms)"],"step_count":2}
{"level":"warn","ts":"2025-01-16T07:22:33.626548Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"164.001675ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-01-16T07:22:33.626708Z","caller":"traceutil/trace.go:171","msg":"trace[679148030] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:9771; }","duration":"164.199077ms","start":"2025-01-16T07:22:33.462468Z","end":"2025-01-16T07:22:33.626667Z","steps":["trace[679148030] 'agreement among raft nodes before linearized reading'  (duration: 163.531393ms)"],"step_count":1}
{"level":"info","ts":"2025-01-16T07:22:33.627668Z","caller":"traceutil/trace.go:171","msg":"trace[1133394836] transaction","detail":"{read_only:false; response_revision:9771; number_of_response:1; }","duration":"198.435036ms","start":"2025-01-16T07:22:33.429149Z","end":"2025-01-16T07:22:33.627584Z","steps":["trace[1133394836] 'process raft request'  (duration: 135.889463ms)","trace[1133394836] 'compare'  (duration: 35.825923ms)"],"step_count":2}
{"level":"info","ts":"2025-01-16T07:23:52.947518Z","caller":"traceutil/trace.go:171","msg":"trace[1610400953] transaction","detail":"{read_only:false; response_revision:9867; number_of_response:1; }","duration":"113.179548ms","start":"2025-01-16T07:23:52.834289Z","end":"2025-01-16T07:23:52.947469Z","steps":["trace[1610400953] 'process raft request'  (duration: 32.322788ms)","trace[1610400953] 'compare'  (duration: 80.188203ms)"],"step_count":2}
{"level":"warn","ts":"2025-01-16T07:24:56.832990Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"515.329344ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-01-16T07:24:56.839506Z","caller":"traceutil/trace.go:171","msg":"trace[178447726] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:9935; }","duration":"521.250357ms","start":"2025-01-16T07:24:56.317623Z","end":"2025-01-16T07:24:56.838874Z","steps":["trace[178447726] 'range keys from in-memory index tree'  (duration: 515.216383ms)"],"step_count":1}
{"level":"warn","ts":"2025-01-16T07:24:56.839739Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-01-16T07:24:56.317546Z","time spent":"522.140739ms","remote":"127.0.0.1:42356","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":29,"request content":"key:\"/registry/health\" "}
{"level":"info","ts":"2025-01-16T07:24:58.403269Z","caller":"traceutil/trace.go:171","msg":"trace[1385662229] transaction","detail":"{read_only:false; response_revision:9937; number_of_response:1; }","duration":"449.175838ms","start":"2025-01-16T07:24:57.954058Z","end":"2025-01-16T07:24:58.403234Z","steps":["trace[1385662229] 'process raft request'  (duration: 394.226553ms)","trace[1385662229] 'compare'  (duration: 48.37282ms)"],"step_count":2}
{"level":"warn","ts":"2025-01-16T07:24:58.406295Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-01-16T07:24:57.954017Z","time spent":"452.144726ms","remote":"127.0.0.1:47414","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":520,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/leases/kube-node-lease/minikube\" mod_revision:9928 > success:<request_put:<key:\"/registry/leases/kube-node-lease/minikube\" value_size:471 >> failure:<request_range:<key:\"/registry/leases/kube-node-lease/minikube\" > >"}
{"level":"warn","ts":"2025-01-16T07:24:58.458997Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"109.667267ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/namespaces/\" range_end:\"/registry/namespaces0\" count_only:true ","response":"range_response_count:0 size:7"}
{"level":"info","ts":"2025-01-16T07:24:58.459357Z","caller":"traceutil/trace.go:171","msg":"trace[1180144705] range","detail":"{range_begin:/registry/namespaces/; range_end:/registry/namespaces0; response_count:0; response_revision:9937; }","duration":"109.97691ms","start":"2025-01-16T07:24:58.349273Z","end":"2025-01-16T07:24:58.459250Z","steps":["trace[1180144705] 'agreement among raft nodes before linearized reading'  (duration: 51.828172ms)","trace[1180144705] 'count revisions from in-memory index tree'  (duration: 57.794688ms)"],"step_count":2}
{"level":"info","ts":"2025-01-16T07:27:22.789779Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":9748}
{"level":"info","ts":"2025-01-16T07:27:22.794753Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":9748,"took":"4.00259ms","hash":772477908,"current-db-size-bytes":1777664,"current-db-size":"1.8 MB","current-db-size-in-use-bytes":901120,"current-db-size-in-use":"901 kB"}
{"level":"info","ts":"2025-01-16T07:27:22.795224Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":772477908,"revision":9748,"compact-revision":9676}
{"level":"info","ts":"2025-01-16T07:32:22.579561Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":10054}
{"level":"info","ts":"2025-01-16T07:32:22.584737Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":10054,"took":"4.647249ms","hash":520075225,"current-db-size-bytes":1777664,"current-db-size":"1.8 MB","current-db-size-in-use-bytes":1073152,"current-db-size-in-use":"1.1 MB"}
{"level":"info","ts":"2025-01-16T07:32:22.584866Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":520075225,"revision":10054,"compact-revision":9748}


==> kernel <==
 20:55:05 up 1 day,  3:50,  0 users,  load average: 0.44, 0.93, 0.98
Linux minikube 5.10.104-linuxkit #1 SMP Thu Mar 17 17:08:06 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.4 LTS"


==> kube-apiserver [135fbef44966] <==
I0220 17:17:34.508330       1 apf_controller.go:377] Starting API Priority and Fairness config controller
I0220 17:17:34.509056       1 customresource_discovery_controller.go:292] Starting DiscoveryController
I0220 17:17:34.509345       1 apiservice_controller.go:100] Starting APIServiceRegistrationController
I0220 17:17:34.509514       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I0220 17:17:34.511717       1 system_namespaces_controller.go:66] Starting system namespaces controller
I0220 17:17:34.512450       1 controller.go:80] Starting OpenAPI V3 AggregationController
I0220 17:17:34.512826       1 local_available_controller.go:156] Starting LocalAvailability controller
I0220 17:17:34.512869       1 cache.go:32] Waiting for caches to sync for LocalAvailability controller
I0220 17:17:34.513435       1 cluster_authentication_trust_controller.go:443] Starting cluster_authentication_trust_controller controller
I0220 17:17:34.513560       1 shared_informer.go:313] Waiting for caches to sync for cluster_authentication_trust_controller
I0220 17:17:34.515387       1 remote_available_controller.go:411] Starting RemoteAvailability controller
I0220 17:17:34.515729       1 cache.go:32] Waiting for caches to sync for RemoteAvailability controller
I0220 17:17:34.544839       1 crdregistration_controller.go:114] Starting crd-autoregister controller
I0220 17:17:34.545206       1 shared_informer.go:313] Waiting for caches to sync for crd-autoregister
I0220 17:17:34.545604       1 controller.go:142] Starting OpenAPI controller
I0220 17:17:34.545919       1 controller.go:90] Starting OpenAPI V3 controller
I0220 17:17:34.546369       1 naming_controller.go:294] Starting NamingConditionController
I0220 17:17:34.546524       1 establishing_controller.go:81] Starting EstablishingController
I0220 17:17:34.546826       1 nonstructuralschema_controller.go:195] Starting NonStructuralSchemaConditionController
I0220 17:17:34.547021       1 apiapproval_controller.go:189] Starting KubernetesAPIApprovalPolicyConformantConditionController
I0220 17:17:34.547330       1 crd_finalizer.go:269] Starting CRDFinalizer
I0220 17:17:34.548363       1 controller.go:119] Starting legacy_token_tracking_controller
I0220 17:17:34.548568       1 shared_informer.go:313] Waiting for caches to sync for configmaps
I0220 17:17:34.548788       1 dynamic_cafile_content.go:160] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0220 17:17:34.549133       1 dynamic_cafile_content.go:160] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0220 17:17:34.808945       1 shared_informer.go:320] Caches are synced for *generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]
I0220 17:17:34.809090       1 policy_source.go:224] refreshing policies
I0220 17:17:34.823541       1 apf_controller.go:382] Running API Priority and Fairness config worker
I0220 17:17:34.823591       1 apf_controller.go:385] Running API Priority and Fairness periodic rebalancing process
I0220 17:17:34.839030       1 shared_informer.go:320] Caches are synced for node_authorizer
I0220 17:17:34.845564       1 shared_informer.go:320] Caches are synced for crd-autoregister
I0220 17:17:34.845750       1 aggregator.go:171] initial CRD sync complete...
I0220 17:17:34.845773       1 autoregister_controller.go:144] Starting autoregister controller
I0220 17:17:34.845782       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0220 17:17:34.849580       1 cache.go:39] Caches are synced for autoregister controller
I0220 17:17:34.849779       1 shared_informer.go:320] Caches are synced for configmaps
I0220 17:17:34.851999       1 controller.go:615] quota admission added evaluator for: leases.coordination.k8s.io
I0220 17:17:34.912690       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0220 17:17:34.918715       1 cache.go:39] Caches are synced for LocalAvailability controller
I0220 17:17:34.927532       1 handler_discovery.go:450] Starting ResourceDiscoveryManager
I0220 17:17:34.930065       1 shared_informer.go:320] Caches are synced for cluster_authentication_trust_controller
I0220 17:17:34.918610       1 cache.go:39] Caches are synced for RemoteAvailability controller
E0220 17:17:35.022186       1 controller.go:97] Error removing old endpoints from kubernetes service: no API server IP addresses were listed in storage, refusing to erase all endpoints for the kubernetes Service
I0220 17:17:35.563076       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I0220 17:17:38.859214       1 controller.go:615] quota admission added evaluator for: endpoints
I0220 17:17:42.505274       1 controller.go:615] quota admission added evaluator for: endpointslices.discovery.k8s.io
E0220 17:17:45.239122       1 controller.go:195] "Failed to update lease" err="Operation cannot be fulfilled on leases.coordination.k8s.io \"apiserver-eqt674mfxb4j56mrjjkoe7b7ii\": StorageError: invalid object, Code: 4, Key: /registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii, ResourceVersion: 0, AdditionalErrorMsg: Precondition failed: UID in precondition: 7cfb02bb-036e-4b53-907a-d4feb6afff8e, UID in object meta: "
I0220 17:32:44.963207       1 alloc.go:330] "allocated clusterIPs" service="default/myapp-service" clusterIPs={"IPv4":"10.102.8.239"}
I0220 17:42:18.357147       1 controller.go:615] quota admission added evaluator for: replicasets.apps
E0220 17:43:40.369720       1 watch.go:250] "Unhandled Error" err="http2: stream closed" logger="UnhandledError"
I0222 20:28:46.701902       1 alloc.go:330] "allocated clusterIPs" service="default/voting-service" clusterIPs={"IPv4":"10.98.196.163"}
I0222 20:33:28.739802       1 alloc.go:330] "allocated clusterIPs" service="default/voting-service" clusterIPs={"IPv4":"10.107.226.248"}
I0222 20:34:57.800845       1 alloc.go:330] "allocated clusterIPs" service="default/voting-service" clusterIPs={"IPv4":"10.103.51.92"}
I0222 20:35:43.117626       1 alloc.go:330] "allocated clusterIPs" service="default/kubernetes" clusterIPs={"IPv4":"10.96.0.1"}
W0222 20:35:43.134150       1 lease.go:265] Resetting endpoints for master service "kubernetes" to [192.168.49.2]
I0222 20:36:55.444936       1 alloc.go:330] "allocated clusterIPs" service="default/voting-service" clusterIPs={"IPv4":"10.96.68.70"}
I0222 20:41:12.305809       1 alloc.go:330] "allocated clusterIPs" service="default/redis" clusterIPs={"IPv4":"10.106.81.199"}
I0222 20:41:56.423137       1 alloc.go:330] "allocated clusterIPs" service="default/db" clusterIPs={"IPv4":"10.111.96.67"}
I0222 20:45:56.335193       1 alloc.go:330] "allocated clusterIPs" service="default/result-service" clusterIPs={"IPv4":"10.104.74.90"}
I0222 20:54:23.209439       1 alloc.go:330] "allocated clusterIPs" service="default/result-service" clusterIPs={"IPv4":"10.105.69.84"}


==> kube-apiserver [fd06917928af] <==
I0114 20:33:47.668922       1 controller.go:90] Starting OpenAPI V3 controller
I0114 20:33:47.668935       1 naming_controller.go:294] Starting NamingConditionController
I0114 20:33:47.668945       1 establishing_controller.go:81] Starting EstablishingController
I0114 20:33:47.668953       1 nonstructuralschema_controller.go:195] Starting NonStructuralSchemaConditionController
I0114 20:33:47.668960       1 apiapproval_controller.go:189] Starting KubernetesAPIApprovalPolicyConformantConditionController
I0114 20:33:47.668968       1 crd_finalizer.go:269] Starting CRDFinalizer
I0114 20:33:47.928013       1 apf_controller.go:382] Running API Priority and Fairness config worker
I0114 20:33:47.928177       1 apf_controller.go:385] Running API Priority and Fairness periodic rebalancing process
I0114 20:33:47.945408       1 cache.go:39] Caches are synced for RemoteAvailability controller
I0114 20:33:47.947417       1 shared_informer.go:320] Caches are synced for crd-autoregister
I0114 20:33:47.954139       1 shared_informer.go:320] Caches are synced for cluster_authentication_trust_controller
I0114 20:33:47.955817       1 aggregator.go:171] initial CRD sync complete...
I0114 20:33:47.956013       1 autoregister_controller.go:144] Starting autoregister controller
I0114 20:33:47.956073       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0114 20:33:47.956111       1 cache.go:39] Caches are synced for autoregister controller
I0114 20:33:47.958239       1 shared_informer.go:320] Caches are synced for node_authorizer
I0114 20:33:47.962061       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0114 20:33:47.966481       1 shared_informer.go:320] Caches are synced for *generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]
I0114 20:33:47.966604       1 policy_source.go:224] refreshing policies
I0114 20:33:47.970931       1 shared_informer.go:320] Caches are synced for configmaps
I0114 20:33:47.973515       1 cache.go:39] Caches are synced for LocalAvailability controller
I0114 20:33:47.973591       1 handler_discovery.go:450] Starting ResourceDiscoveryManager
I0114 20:33:47.980424       1 controller.go:615] quota admission added evaluator for: namespaces
E0114 20:33:48.076453       1 controller.go:145] "Failed to ensure lease exists, will retry" err="namespaces \"kube-system\" not found" interval="200ms"
E0114 20:33:48.089813       1 controller.go:148] "Unhandled Error" err="while syncing ConfigMap \"kube-system/kube-apiserver-legacy-service-account-token-tracking\", err: namespaces \"kube-system\" not found" logger="UnhandledError"
I0114 20:33:48.305510       1 controller.go:615] quota admission added evaluator for: leases.coordination.k8s.io
I0114 20:33:48.697416       1 storage_scheduling.go:95] created PriorityClass system-node-critical with value 2000001000
I0114 20:33:48.719674       1 storage_scheduling.go:95] created PriorityClass system-cluster-critical with value 2000000000
I0114 20:33:48.719789       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I0114 20:33:50.156146       1 controller.go:615] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I0114 20:33:50.240858       1 controller.go:615] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I0114 20:33:50.360832       1 alloc.go:330] "allocated clusterIPs" service="default/kubernetes" clusterIPs={"IPv4":"10.96.0.1"}
W0114 20:33:50.375938       1 lease.go:265] Resetting endpoints for master service "kubernetes" to [192.168.49.2]
I0114 20:33:50.377007       1 controller.go:615] quota admission added evaluator for: endpoints
I0114 20:33:50.388340       1 controller.go:615] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0114 20:33:51.011907       1 controller.go:615] quota admission added evaluator for: serviceaccounts
I0114 20:33:51.217338       1 controller.go:615] quota admission added evaluator for: deployments.apps
I0114 20:33:51.278957       1 alloc.go:330] "allocated clusterIPs" service="kube-system/kube-dns" clusterIPs={"IPv4":"10.96.0.10"}
I0114 20:33:51.341054       1 controller.go:615] quota admission added evaluator for: daemonsets.apps
I0114 20:33:56.537265       1 controller.go:615] quota admission added evaluator for: replicasets.apps
I0114 20:33:56.741250       1 controller.go:615] quota admission added evaluator for: controllerrevisions.apps
E0115 03:32:28.304708       1 controller.go:195] "Failed to update lease" err="Operation cannot be fulfilled on leases.coordination.k8s.io \"apiserver-eqt674mfxb4j56mrjjkoe7b7ii\": StorageError: invalid object, Code: 4, Key: /registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii, ResourceVersion: 0, AdditionalErrorMsg: Precondition failed: UID in precondition: 8c941d4e-7947-4886-9cd6-f16497014079, UID in object meta: "
E0115 05:33:18.208275       1 controller.go:195] "Failed to update lease" err="Operation cannot be fulfilled on leases.coordination.k8s.io \"apiserver-eqt674mfxb4j56mrjjkoe7b7ii\": StorageError: invalid object, Code: 4, Key: /registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii, ResourceVersion: 0, AdditionalErrorMsg: Precondition failed: UID in precondition: 86cdcf91-a4ed-4e59-8feb-19138a66557c, UID in object meta: "
E0115 07:02:15.922327       1 controller.go:195] "Failed to update lease" err="Operation cannot be fulfilled on leases.coordination.k8s.io \"apiserver-eqt674mfxb4j56mrjjkoe7b7ii\": StorageError: invalid object, Code: 4, Key: /registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii, ResourceVersion: 0, AdditionalErrorMsg: Precondition failed: UID in precondition: 24db375f-8d40-45b8-9790-5cdc32ea8522, UID in object meta: "
E0115 10:02:18.437635       1 controller.go:195] "Failed to update lease" err="Operation cannot be fulfilled on leases.coordination.k8s.io \"apiserver-eqt674mfxb4j56mrjjkoe7b7ii\": StorageError: invalid object, Code: 4, Key: /registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii, ResourceVersion: 0, AdditionalErrorMsg: Precondition failed: UID in precondition: 765541bf-c199-47f4-ad2c-7a8e631d161d, UID in object meta: "
E0116 02:29:16.556612       1 repair.go:127] "Unhandled Error" err="unable to refresh the service IP block: rpc error: code = Unavailable desc = error reading from server: read tcp 127.0.0.1:48182->127.0.0.1:2379: read: connection timed out" logger="UnhandledError"
E0116 02:29:16.560835       1 repair.go:85] "Unhandled Error" err="unable to refresh the port allocations: rpc error: code = Unavailable desc = error reading from server: read tcp 127.0.0.1:46872->127.0.0.1:2379: read: connection timed out" logger="UnhandledError"
E0116 02:29:16.744061       1 status.go:71] "Unhandled Error" err="apiserver received an error that is not an metav1.Status: &errors.errorString{s:\"context canceled\"}: context canceled" logger="UnhandledError"
E0116 02:29:16.801267       1 writers.go:122] "Unhandled Error" err="apiserver was unable to write a JSON response: http: Handler timeout" logger="UnhandledError"
E0116 02:29:16.802346       1 status.go:71] "Unhandled Error" err="apiserver received an error that is not an metav1.Status: &errors.errorString{s:\"http: Handler timeout\"}: http: Handler timeout" logger="UnhandledError"
E0116 02:29:16.805505       1 writers.go:135] "Unhandled Error" err="apiserver was unable to write a fallback JSON response: http: Handler timeout" logger="UnhandledError"
E0116 02:29:16.806737       1 timeout.go:140] "Post-timeout activity" logger="UnhandledError" timeElapsed="5.676267ms" method="GET" path="/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath" result=null
E0116 02:29:17.394131       1 status.go:71] "Unhandled Error" err="apiserver received an error that is not an metav1.Status: &errors.errorString{s:\"client disconnected\"}: client disconnected" logger="UnhandledError"
E0116 02:29:17.397152       1 writers.go:122] "Unhandled Error" err="apiserver was unable to write a JSON response: http: Handler timeout" logger="UnhandledError"
E0116 02:29:17.399606       1 status.go:71] "Unhandled Error" err="apiserver received an error that is not an metav1.Status: &errors.errorString{s:\"http: Handler timeout\"}: http: Handler timeout" logger="UnhandledError"
E0116 02:29:17.406021       1 writers.go:135] "Unhandled Error" err="apiserver was unable to write a fallback JSON response: http: Handler timeout" logger="UnhandledError"
E0116 02:29:17.410892       1 timeout.go:140] "Post-timeout activity" logger="UnhandledError" timeElapsed="17.511977ms" method="POST" path="/api/v1/namespaces/default/events" result=null
E0116 02:29:27.552453       1 controller.go:195] "Failed to update lease" err="Operation cannot be fulfilled on leases.coordination.k8s.io \"apiserver-eqt674mfxb4j56mrjjkoe7b7ii\": StorageError: invalid object, Code: 4, Key: /registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii, ResourceVersion: 0, AdditionalErrorMsg: Precondition failed: UID in precondition: 14fd1de0-3102-46e8-8088-415600545f96, UID in object meta: "
E0116 05:21:46.388688       1 controller.go:195] "Failed to update lease" err="Operation cannot be fulfilled on leases.coordination.k8s.io \"apiserver-eqt674mfxb4j56mrjjkoe7b7ii\": StorageError: invalid object, Code: 4, Key: /registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii, ResourceVersion: 0, AdditionalErrorMsg: Precondition failed: UID in precondition: 7139f154-b4c0-4d83-90af-8dd809907eb0, UID in object meta: "
E0116 07:22:32.936388       1 controller.go:195] "Failed to update lease" err="Operation cannot be fulfilled on leases.coordination.k8s.io \"apiserver-eqt674mfxb4j56mrjjkoe7b7ii\": StorageError: invalid object, Code: 4, Key: /registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii, ResourceVersion: 0, AdditionalErrorMsg: Precondition failed: UID in precondition: c4b481a2-811d-444c-ab7f-da89aabf7871, UID in object meta: "


==> kube-controller-manager [214b15d6fd40] <==
I0116 03:05:12.431545       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0116 03:05:53.334520       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-5684855748" duration="100.836µs"
I0116 03:06:00.317115       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-5684855748" duration="36.047µs"
I0116 03:06:05.318208       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-5684855748" duration="82.445µs"
I0116 03:06:12.329208       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-5684855748" duration="92.464µs"
I0116 03:10:17.588910       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0116 03:10:53.117099       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-5684855748" duration="83.659µs"
I0116 03:11:07.105821       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-5684855748" duration="122.835µs"
I0116 03:11:14.098698       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-5684855748" duration="79.398µs"
I0116 03:11:25.072896       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-5684855748" duration="72.969µs"
I0116 03:15:22.285581       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0116 03:15:58.872444       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-5684855748" duration="89.992µs"
I0116 03:16:12.867072       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-5684855748" duration="50.504µs"
I0116 03:16:18.911659       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-5684855748" duration="175.902µs"
I0116 03:16:29.861194       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-5684855748" duration="55.338µs"
I0116 04:11:19.488121       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0116 04:11:19.693551       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0116 04:11:22.480830       1 node_lifecycle_controller.go:1036] "Controller detected that all Nodes are not-Ready. Entering master disruption mode" logger="node-lifecycle-controller"
I0116 04:11:32.052684       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0116 04:11:32.483586       1 node_lifecycle_controller.go:1055] "Controller detected that some Nodes are Ready. Exiting master disruption mode" logger="node-lifecycle-controller"
I0116 04:11:33.061060       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0116 04:11:35.102459       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-5684855748" duration="215.582µs"
I0116 04:11:35.194230       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-5684855748" duration="117.999µs"
I0116 04:12:01.366853       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-5684855748" duration="442.831µs"
I0116 04:12:07.345325       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-5684855748" duration="87.814µs"
I0116 05:21:36.424622       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-5684855748" duration="535.221µs"
I0116 05:21:36.451082       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0116 05:21:36.954476       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-5684855748" duration="66.386µs"
E0116 05:21:36.983899       1 node_lifecycle_controller.go:978] "Error updating node" err="Operation cannot be fulfilled on nodes \"minikube\": the object has been modified; please apply your changes to the latest version and try again" logger="node-lifecycle-controller" node="minikube"
I0116 05:22:17.016808       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-5684855748" duration="119.404µs"
I0116 05:22:22.980828       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-5684855748" duration="92.421µs"
I0116 07:22:22.576379       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0116 07:22:22.878632       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-5684855748" duration="85.946µs"
E0116 07:22:22.958007       1 node_lifecycle_controller.go:978] "Error updating node" err="Operation cannot be fulfilled on nodes \"minikube\": the object has been modified; please apply your changes to the latest version and try again" logger="node-lifecycle-controller" node="minikube"
I0116 07:22:22.960370       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-5684855748" duration="52.22µs"
I0116 07:22:22.999593       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0116 07:22:23.012377       1 node_lifecycle_controller.go:1036] "Controller detected that all Nodes are not-Ready. Entering master disruption mode" logger="node-lifecycle-controller"
I0116 07:22:33.518199       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0116 07:22:33.999637       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0116 07:22:38.022643       1 node_lifecycle_controller.go:1055] "Controller detected that some Nodes are Ready. Exiting master disruption mode" logger="node-lifecycle-controller"
I0116 07:23:04.453761       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-5684855748" duration="158.078µs"
I0116 07:23:06.498417       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-5684855748" duration="413.175µs"
I0116 07:23:17.405435       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-5684855748" duration="148.344µs"
I0116 07:23:18.393513       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-5684855748" duration="156.587µs"
I0116 07:23:31.400211       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-5684855748" duration="130.463µs"
I0116 07:23:33.515184       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-5684855748" duration="133.421µs"
I0116 07:24:02.324463       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-5684855748" duration="95.613µs"
I0116 07:24:13.377464       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-5684855748" duration="90.377µs"
I0116 07:24:15.425508       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-5684855748" duration="120.51µs"
I0116 07:24:26.299607       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-5684855748" duration="113.542µs"
I0116 07:25:23.273556       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-5684855748" duration="223.671µs"
I0116 07:25:35.260779       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-5684855748" duration="210.224µs"
I0116 07:25:38.250359       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-5684855748" duration="88.919µs"
I0116 07:25:48.253031       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-5684855748" duration="183.291µs"
I0116 07:27:39.865009       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0116 07:28:13.132468       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-5684855748" duration="317.032µs"
I0116 07:28:18.135455       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-5684855748" duration="75.859µs"
I0116 07:28:27.113107       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-5684855748" duration="103.147µs"
I0116 07:28:32.105194       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-5684855748" duration="117.146µs"
I0116 07:32:46.397675       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"


==> kube-controller-manager [57234d3f79c1] <==
I0220 17:43:29.272039       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-5684855748" duration="215.453µs"
I0220 17:43:29.317213       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-5684855748" duration="105.361µs"
I0220 17:43:29.510763       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-5684855748" duration="45.76µs"
I0220 17:43:30.032513       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-5684855748" duration="47.102µs"
I0220 17:43:30.040891       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-5684855748" duration="71.11µs"
I0220 17:43:30.051401       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-5684855748" duration="48.218µs"
I0220 17:43:31.052890       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-5684855748" duration="50.575µs"
I0220 17:43:32.071901       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-5684855748" duration="54.817µs"
I0220 17:43:32.128219       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-5684855748" duration="42.054µs"
I0220 17:43:38.077761       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-5684855748" duration="60.417µs"
I0220 17:43:39.962558       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-5684855748" duration="107.064406ms"
I0220 17:43:39.963039       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-5684855748" duration="350.583µs"
I0220 17:43:40.000268       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-5684855748" duration="28.978021ms"
I0220 17:43:40.000355       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-5684855748" duration="52.9µs"
I0220 17:43:40.024631       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-5684855748" duration="34.366µs"
I0220 17:43:40.121739       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-5684855748" duration="258.075µs"
I0220 17:43:40.339402       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-5684855748" duration="147.507µs"
I0220 17:43:40.355135       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-5684855748" duration="44.892µs"
I0220 17:43:40.366313       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-5684855748" duration="61.024µs"
I0220 17:43:42.351858       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-5684855748" duration="46.991µs"
I0220 17:43:46.076479       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-5684855748" duration="46.996µs"
I0220 17:43:52.079037       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-5684855748" duration="44.974µs"
I0220 17:43:57.069699       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-5684855748" duration="52.029µs"
I0220 17:43:58.078653       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-5684855748" duration="58.402µs"
I0220 17:43:58.092056       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-5684855748" duration="97.047µs"
I0220 17:43:59.977679       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0220 17:44:03.062539       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-5684855748" duration="145.833µs"
I0220 17:44:10.083193       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-5684855748" duration="146.32µs"
I0220 17:44:12.568983       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-5684855748" duration="9.011µs"
I0220 17:49:06.808911       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0220 17:54:13.530647       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0220 17:59:20.628720       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0220 18:04:27.238613       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0220 18:09:34.931315       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0220 18:40:27.570518       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
E0220 18:40:27.924629       1 node_lifecycle_controller.go:978] "Error updating node" err="Operation cannot be fulfilled on nodes \"minikube\": the object has been modified; please apply your changes to the latest version and try again" logger="node-lifecycle-controller" node="minikube"
I0220 18:47:01.661414       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0220 18:47:01.731444       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0220 18:47:01.900485       1 node_lifecycle_controller.go:1036] "Controller detected that all Nodes are not-Ready. Entering master disruption mode" logger="node-lifecycle-controller"
I0220 18:47:01.911913       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="53.749893ms"
I0220 18:47:01.912795       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="216.429µs"
I0220 18:47:01.995600       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0220 18:47:09.522544       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="11.056812ms"
I0220 18:47:09.523332       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="39.755µs"
I0220 18:47:13.116421       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0220 18:47:13.730064       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0220 18:47:16.918634       1 node_lifecycle_controller.go:1055] "Controller detected that some Nodes are Ready. Exiting master disruption mode" logger="node-lifecycle-controller"
I0222 20:23:42.363095       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0222 20:23:42.467799       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="19.126952ms"
I0222 20:23:44.116091       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
E0222 20:23:44.240430       1 node_lifecycle_controller.go:978] "Error updating node" err="Operation cannot be fulfilled on nodes \"minikube\": the object has been modified; please apply your changes to the latest version and try again" logger="node-lifecycle-controller" node="minikube"
I0222 20:28:51.246977       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0222 20:29:21.826913       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0222 20:34:28.165733       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0222 20:39:34.138573       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0222 20:41:16.316095       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0222 20:42:17.201458       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0222 20:46:52.947552       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0222 20:47:13.296590       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0222 20:52:19.170883       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"


==> kube-proxy [6746c867c24a] <==
I0114 20:33:58.335240       1 server_linux.go:66] "Using iptables proxy"
I0114 20:33:58.524150       1 server.go:677] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
E0114 20:33:58.524294       1 server.go:234] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I0114 20:33:58.725364       1 server.go:243] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0114 20:33:58.725658       1 server_linux.go:169] "Using iptables Proxier"
I0114 20:33:58.736382       1 proxier.go:255] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
I0114 20:33:58.737351       1 server.go:483] "Version info" version="v1.31.0"
I0114 20:33:58.738014       1 server.go:485] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0114 20:33:58.741020       1 config.go:197] "Starting service config controller"
I0114 20:33:58.741225       1 shared_informer.go:313] Waiting for caches to sync for service config
I0114 20:33:58.741333       1 config.go:104] "Starting endpoint slice config controller"
I0114 20:33:58.741345       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I0114 20:33:58.744217       1 config.go:326] "Starting node config controller"
I0114 20:33:58.744356       1 shared_informer.go:313] Waiting for caches to sync for node config
I0114 20:33:58.841775       1 shared_informer.go:320] Caches are synced for endpoint slice config
I0114 20:33:58.841846       1 shared_informer.go:320] Caches are synced for service config
I0114 20:33:58.845236       1 shared_informer.go:320] Caches are synced for node config


==> kube-proxy [8098f4cd9060] <==
I0220 17:17:44.253071       1 server_linux.go:66] "Using iptables proxy"
I0220 17:17:44.524003       1 server.go:677] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
E0220 17:17:44.524244       1 server.go:234] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I0220 17:17:44.649213       1 server.go:243] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0220 17:17:44.649342       1 server_linux.go:169] "Using iptables Proxier"
I0220 17:17:44.658495       1 proxier.go:255] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
I0220 17:17:44.669565       1 server.go:483] "Version info" version="v1.31.0"
I0220 17:17:44.669937       1 server.go:485] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0220 17:17:44.721162       1 config.go:197] "Starting service config controller"
I0220 17:17:44.724919       1 shared_informer.go:313] Waiting for caches to sync for service config
I0220 17:17:44.727616       1 config.go:104] "Starting endpoint slice config controller"
I0220 17:17:44.737155       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I0220 17:17:44.739552       1 config.go:326] "Starting node config controller"
I0220 17:17:44.740586       1 shared_informer.go:313] Waiting for caches to sync for node config
I0220 17:17:44.827883       1 shared_informer.go:320] Caches are synced for service config
I0220 17:17:44.841477       1 shared_informer.go:320] Caches are synced for node config
I0220 17:17:44.842053       1 shared_informer.go:320] Caches are synced for endpoint slice config


==> kube-scheduler [c79afb36d1da] <==
I0220 17:17:32.085484       1 serving.go:386] Generated self-signed cert in-memory
W0220 17:17:34.649158       1 requestheader_controller.go:196] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0220 17:17:34.649202       1 authentication.go:370] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0220 17:17:34.649245       1 authentication.go:371] Continuing without authentication configuration. This may treat all requests as anonymous.
W0220 17:17:34.649258       1 authentication.go:372] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0220 17:17:34.880360       1 server.go:167] "Starting Kubernetes Scheduler" version="v1.31.0"
I0220 17:17:34.881361       1 server.go:169] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0220 17:17:34.896039       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0220 17:17:34.896409       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0220 17:17:34.897261       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I0220 17:17:34.898022       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0220 17:17:35.000084       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file


==> kube-scheduler [fc63264d4492] <==
I0114 20:33:47.762230       1 serving.go:386] Generated self-signed cert in-memory
W0114 20:33:49.613922       1 requestheader_controller.go:196] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0114 20:33:49.614109       1 authentication.go:370] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0114 20:33:49.614292       1 authentication.go:371] Continuing without authentication configuration. This may treat all requests as anonymous.
W0114 20:33:49.614375       1 authentication.go:372] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0114 20:33:49.652564       1 server.go:167] "Starting Kubernetes Scheduler" version="v1.31.0"
I0114 20:33:49.652728       1 server.go:169] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0114 20:33:49.666453       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0114 20:33:49.667187       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0114 20:33:49.671139       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0114 20:33:49.667295       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
W0114 20:33:49.680593       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E0114 20:33:49.682709       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0114 20:33:49.681621       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E0114 20:33:49.683069       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W0114 20:33:49.681742       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E0114 20:33:49.683246       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0114 20:33:49.681908       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E0114 20:33:49.683376       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0114 20:33:49.690389       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E0114 20:33:49.694216       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0114 20:33:49.693161       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E0114 20:33:49.698560       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0114 20:33:49.693361       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E0114 20:33:49.698994       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0114 20:33:49.693569       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E0114 20:33:49.699746       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W0114 20:33:49.701033       1 reflector.go:561] runtime/asm_amd64.s:1695: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
W0114 20:33:49.693781       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
W0114 20:33:49.693946       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E0114 20:33:49.702731       1 reflector.go:158] "Unhandled Error" err="runtime/asm_amd64.s:1695: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError"
E0114 20:33:49.702835       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
E0114 20:33:49.702957       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0114 20:33:49.694081       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
W0114 20:33:49.701580       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
W0114 20:33:49.701726       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
W0114 20:33:49.702169       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E0114 20:33:49.704658       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError"
E0114 20:33:49.704719       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
E0114 20:33:49.704802       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError"
E0114 20:33:49.706555       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError"
I0114 20:33:50.779752       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file


==> kubelet <==
Feb 22 20:50:47 minikube kubelet[1384]: I0222 20:50:47.535235    1384 scope.go:117] "RemoveContainer" containerID="4e132b1450258265a55ae3b12ce8016d5a4864c15166604007dc5f50c3ae7478"
Feb 22 20:50:47 minikube kubelet[1384]: I0222 20:50:47.535554    1384 scope.go:117] "RemoveContainer" containerID="9c93f17d95e5a38002c259599664fd565a994fe0392b07f426822c281ead874d"
Feb 22 20:50:47 minikube kubelet[1384]: E0222 20:50:47.535755    1384 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"postgres\" with CrashLoopBackOff: \"back-off 10s restarting failed container=postgres pod=postgres-pod_default(8cc7dbd7-594c-4788-a010-1b3187361416)\"" pod="default/postgres-pod" podUID="8cc7dbd7-594c-4788-a010-1b3187361416"
Feb 22 20:50:48 minikube kubelet[1384]: I0222 20:50:48.577582    1384 scope.go:117] "RemoveContainer" containerID="9c93f17d95e5a38002c259599664fd565a994fe0392b07f426822c281ead874d"
Feb 22 20:50:48 minikube kubelet[1384]: E0222 20:50:48.577766    1384 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"postgres\" with CrashLoopBackOff: \"back-off 10s restarting failed container=postgres pod=postgres-pod_default(8cc7dbd7-594c-4788-a010-1b3187361416)\"" pod="default/postgres-pod" podUID="8cc7dbd7-594c-4788-a010-1b3187361416"
Feb 22 20:50:59 minikube kubelet[1384]: I0222 20:50:59.966303    1384 scope.go:117] "RemoveContainer" containerID="9c93f17d95e5a38002c259599664fd565a994fe0392b07f426822c281ead874d"
Feb 22 20:51:00 minikube kubelet[1384]: I0222 20:51:00.833486    1384 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="default/postgres-pod" podStartSLOduration=1.584512482 podStartE2EDuration="16.833470282s" podCreationTimestamp="2025-02-22 20:50:44 +0000 UTC" firstStartedPulling="2025-02-22 20:50:45.400701532 +0000 UTC m=+99877.339067204" lastFinishedPulling="2025-02-22 20:51:00.629437507 +0000 UTC m=+99892.588025004" observedRunningTime="2025-02-22 20:51:00.833081848 +0000 UTC m=+99892.791669356" watchObservedRunningTime="2025-02-22 20:51:00.833470282 +0000 UTC m=+99892.792057782"
Feb 22 20:51:01 minikube kubelet[1384]: I0222 20:51:01.865121    1384 scope.go:117] "RemoveContainer" containerID="9c93f17d95e5a38002c259599664fd565a994fe0392b07f426822c281ead874d"
Feb 22 20:51:01 minikube kubelet[1384]: I0222 20:51:01.868085    1384 scope.go:117] "RemoveContainer" containerID="5dea819f4bb20f75dada7b0b89a7143803da2b3555413b14e85e0369817796a2"
Feb 22 20:51:01 minikube kubelet[1384]: E0222 20:51:01.868696    1384 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"postgres\" with CrashLoopBackOff: \"back-off 20s restarting failed container=postgres pod=postgres-pod_default(8cc7dbd7-594c-4788-a010-1b3187361416)\"" pod="default/postgres-pod" podUID="8cc7dbd7-594c-4788-a010-1b3187361416"
Feb 22 20:51:15 minikube kubelet[1384]: I0222 20:51:15.965128    1384 scope.go:117] "RemoveContainer" containerID="5dea819f4bb20f75dada7b0b89a7143803da2b3555413b14e85e0369817796a2"
Feb 22 20:51:15 minikube kubelet[1384]: E0222 20:51:15.965541    1384 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"postgres\" with CrashLoopBackOff: \"back-off 20s restarting failed container=postgres pod=postgres-pod_default(8cc7dbd7-594c-4788-a010-1b3187361416)\"" pod="default/postgres-pod" podUID="8cc7dbd7-594c-4788-a010-1b3187361416"
Feb 22 20:51:28 minikube kubelet[1384]: I0222 20:51:28.946976    1384 scope.go:117] "RemoveContainer" containerID="5dea819f4bb20f75dada7b0b89a7143803da2b3555413b14e85e0369817796a2"
Feb 22 20:51:30 minikube kubelet[1384]: I0222 20:51:30.443149    1384 scope.go:117] "RemoveContainer" containerID="5dea819f4bb20f75dada7b0b89a7143803da2b3555413b14e85e0369817796a2"
Feb 22 20:51:30 minikube kubelet[1384]: I0222 20:51:30.444313    1384 scope.go:117] "RemoveContainer" containerID="d83e8b3e33375f6e2bccccf68ab0081e5b2e15875f6d7c6b9ecf659ae21bd831"
Feb 22 20:51:30 minikube kubelet[1384]: E0222 20:51:30.444454    1384 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"postgres\" with CrashLoopBackOff: \"back-off 40s restarting failed container=postgres pod=postgres-pod_default(8cc7dbd7-594c-4788-a010-1b3187361416)\"" pod="default/postgres-pod" podUID="8cc7dbd7-594c-4788-a010-1b3187361416"
Feb 22 20:51:43 minikube kubelet[1384]: I0222 20:51:43.945491    1384 scope.go:117] "RemoveContainer" containerID="d83e8b3e33375f6e2bccccf68ab0081e5b2e15875f6d7c6b9ecf659ae21bd831"
Feb 22 20:51:43 minikube kubelet[1384]: E0222 20:51:43.947268    1384 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"postgres\" with CrashLoopBackOff: \"back-off 40s restarting failed container=postgres pod=postgres-pod_default(8cc7dbd7-594c-4788-a010-1b3187361416)\"" pod="default/postgres-pod" podUID="8cc7dbd7-594c-4788-a010-1b3187361416"
Feb 22 20:51:54 minikube kubelet[1384]: I0222 20:51:54.924741    1384 scope.go:117] "RemoveContainer" containerID="d83e8b3e33375f6e2bccccf68ab0081e5b2e15875f6d7c6b9ecf659ae21bd831"
Feb 22 20:51:54 minikube kubelet[1384]: E0222 20:51:54.925141    1384 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"postgres\" with CrashLoopBackOff: \"back-off 40s restarting failed container=postgres pod=postgres-pod_default(8cc7dbd7-594c-4788-a010-1b3187361416)\"" pod="default/postgres-pod" podUID="8cc7dbd7-594c-4788-a010-1b3187361416"
Feb 22 20:52:07 minikube kubelet[1384]: I0222 20:52:07.924210    1384 scope.go:117] "RemoveContainer" containerID="d83e8b3e33375f6e2bccccf68ab0081e5b2e15875f6d7c6b9ecf659ae21bd831"
Feb 22 20:52:07 minikube kubelet[1384]: E0222 20:52:07.924789    1384 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"postgres\" with CrashLoopBackOff: \"back-off 40s restarting failed container=postgres pod=postgres-pod_default(8cc7dbd7-594c-4788-a010-1b3187361416)\"" pod="default/postgres-pod" podUID="8cc7dbd7-594c-4788-a010-1b3187361416"
Feb 22 20:52:18 minikube kubelet[1384]: I0222 20:52:18.929023    1384 scope.go:117] "RemoveContainer" containerID="d83e8b3e33375f6e2bccccf68ab0081e5b2e15875f6d7c6b9ecf659ae21bd831"
Feb 22 20:52:20 minikube kubelet[1384]: I0222 20:52:20.515655    1384 scope.go:117] "RemoveContainer" containerID="d83e8b3e33375f6e2bccccf68ab0081e5b2e15875f6d7c6b9ecf659ae21bd831"
Feb 22 20:52:20 minikube kubelet[1384]: I0222 20:52:20.516661    1384 scope.go:117] "RemoveContainer" containerID="d95e8f4a14690f6646904f72989fe201c05a81cd8741143cac64907ab832553f"
Feb 22 20:52:20 minikube kubelet[1384]: E0222 20:52:20.516857    1384 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"postgres\" with CrashLoopBackOff: \"back-off 1m20s restarting failed container=postgres pod=postgres-pod_default(8cc7dbd7-594c-4788-a010-1b3187361416)\"" pod="default/postgres-pod" podUID="8cc7dbd7-594c-4788-a010-1b3187361416"
Feb 22 20:52:30 minikube kubelet[1384]: I0222 20:52:30.903172    1384 scope.go:117] "RemoveContainer" containerID="d95e8f4a14690f6646904f72989fe201c05a81cd8741143cac64907ab832553f"
Feb 22 20:52:30 minikube kubelet[1384]: E0222 20:52:30.904295    1384 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"postgres\" with CrashLoopBackOff: \"back-off 1m20s restarting failed container=postgres pod=postgres-pod_default(8cc7dbd7-594c-4788-a010-1b3187361416)\"" pod="default/postgres-pod" podUID="8cc7dbd7-594c-4788-a010-1b3187361416"
Feb 22 20:52:42 minikube kubelet[1384]: I0222 20:52:42.903908    1384 scope.go:117] "RemoveContainer" containerID="d95e8f4a14690f6646904f72989fe201c05a81cd8741143cac64907ab832553f"
Feb 22 20:52:42 minikube kubelet[1384]: E0222 20:52:42.905404    1384 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"postgres\" with CrashLoopBackOff: \"back-off 1m20s restarting failed container=postgres pod=postgres-pod_default(8cc7dbd7-594c-4788-a010-1b3187361416)\"" pod="default/postgres-pod" podUID="8cc7dbd7-594c-4788-a010-1b3187361416"
Feb 22 20:52:57 minikube kubelet[1384]: I0222 20:52:57.882129    1384 scope.go:117] "RemoveContainer" containerID="d95e8f4a14690f6646904f72989fe201c05a81cd8741143cac64907ab832553f"
Feb 22 20:52:57 minikube kubelet[1384]: E0222 20:52:57.882945    1384 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"postgres\" with CrashLoopBackOff: \"back-off 1m20s restarting failed container=postgres pod=postgres-pod_default(8cc7dbd7-594c-4788-a010-1b3187361416)\"" pod="default/postgres-pod" podUID="8cc7dbd7-594c-4788-a010-1b3187361416"
Feb 22 20:53:10 minikube kubelet[1384]: I0222 20:53:10.882232    1384 scope.go:117] "RemoveContainer" containerID="d95e8f4a14690f6646904f72989fe201c05a81cd8741143cac64907ab832553f"
Feb 22 20:53:10 minikube kubelet[1384]: E0222 20:53:10.883149    1384 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"postgres\" with CrashLoopBackOff: \"back-off 1m20s restarting failed container=postgres pod=postgres-pod_default(8cc7dbd7-594c-4788-a010-1b3187361416)\"" pod="default/postgres-pod" podUID="8cc7dbd7-594c-4788-a010-1b3187361416"
Feb 22 20:53:23 minikube kubelet[1384]: I0222 20:53:23.861723    1384 scope.go:117] "RemoveContainer" containerID="d95e8f4a14690f6646904f72989fe201c05a81cd8741143cac64907ab832553f"
Feb 22 20:53:23 minikube kubelet[1384]: E0222 20:53:23.863156    1384 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"postgres\" with CrashLoopBackOff: \"back-off 1m20s restarting failed container=postgres pod=postgres-pod_default(8cc7dbd7-594c-4788-a010-1b3187361416)\"" pod="default/postgres-pod" podUID="8cc7dbd7-594c-4788-a010-1b3187361416"
Feb 22 20:53:26 minikube kubelet[1384]: E0222 20:53:26.524319    1384 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="53c9352b-cab1-4c87-9162-04e3051bbbb1" containerName="postgres"
Feb 22 20:53:26 minikube kubelet[1384]: I0222 20:53:26.524860    1384 memory_manager.go:354] "RemoveStaleState removing state" podUID="53c9352b-cab1-4c87-9162-04e3051bbbb1" containerName="postgres"
Feb 22 20:53:26 minikube kubelet[1384]: I0222 20:53:26.524952    1384 memory_manager.go:354] "RemoveStaleState removing state" podUID="53c9352b-cab1-4c87-9162-04e3051bbbb1" containerName="postgres"
Feb 22 20:53:26 minikube kubelet[1384]: I0222 20:53:26.610660    1384 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-zpwnp\" (UniqueName: \"kubernetes.io/projected/6c802eb6-1017-48d9-b52b-9ff781009b4b-kube-api-access-zpwnp\") pod \"result-app-pod\" (UID: \"6c802eb6-1017-48d9-b52b-9ff781009b4b\") " pod="default/result-app-pod"
Feb 22 20:53:27 minikube kubelet[1384]: I0222 20:53:27.223848    1384 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="e0ab6f6c842bc16ae341fe741078485241a7d2d3a6a9cab4ff8e5b9b3eef5139"
Feb 22 20:53:35 minikube kubelet[1384]: I0222 20:53:35.862184    1384 scope.go:117] "RemoveContainer" containerID="d95e8f4a14690f6646904f72989fe201c05a81cd8741143cac64907ab832553f"
Feb 22 20:53:35 minikube kubelet[1384]: E0222 20:53:35.863253    1384 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"postgres\" with CrashLoopBackOff: \"back-off 1m20s restarting failed container=postgres pod=postgres-pod_default(8cc7dbd7-594c-4788-a010-1b3187361416)\"" pod="default/postgres-pod" podUID="8cc7dbd7-594c-4788-a010-1b3187361416"
Feb 22 20:53:48 minikube kubelet[1384]: I0222 20:53:48.865602    1384 scope.go:117] "RemoveContainer" containerID="d95e8f4a14690f6646904f72989fe201c05a81cd8741143cac64907ab832553f"
Feb 22 20:53:50 minikube kubelet[1384]: I0222 20:53:50.808066    1384 scope.go:117] "RemoveContainer" containerID="d95e8f4a14690f6646904f72989fe201c05a81cd8741143cac64907ab832553f"
Feb 22 20:53:50 minikube kubelet[1384]: I0222 20:53:50.808402    1384 scope.go:117] "RemoveContainer" containerID="f87d4c121dedd51e95e3e29f848650ad3e6741b2ecbf167353f7de90ca011be0"
Feb 22 20:53:50 minikube kubelet[1384]: E0222 20:53:50.808545    1384 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"postgres\" with CrashLoopBackOff: \"back-off 2m40s restarting failed container=postgres pod=postgres-pod_default(8cc7dbd7-594c-4788-a010-1b3187361416)\"" pod="default/postgres-pod" podUID="8cc7dbd7-594c-4788-a010-1b3187361416"
Feb 22 20:53:50 minikube kubelet[1384]: I0222 20:53:50.820830    1384 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="default/result-app-pod" podStartSLOduration=24.82081357 podStartE2EDuration="24.82081357s" podCreationTimestamp="2025-02-22 20:53:26 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-02-22 20:53:28.26788303 +0000 UTC m=+100040.330070053" watchObservedRunningTime="2025-02-22 20:53:50.82081357 +0000 UTC m=+100062.883000590"
Feb 22 20:54:02 minikube kubelet[1384]: I0222 20:54:02.842543    1384 scope.go:117] "RemoveContainer" containerID="f87d4c121dedd51e95e3e29f848650ad3e6741b2ecbf167353f7de90ca011be0"
Feb 22 20:54:02 minikube kubelet[1384]: E0222 20:54:02.843890    1384 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"postgres\" with CrashLoopBackOff: \"back-off 2m40s restarting failed container=postgres pod=postgres-pod_default(8cc7dbd7-594c-4788-a010-1b3187361416)\"" pod="default/postgres-pod" podUID="8cc7dbd7-594c-4788-a010-1b3187361416"
Feb 22 20:54:13 minikube kubelet[1384]: I0222 20:54:13.840880    1384 scope.go:117] "RemoveContainer" containerID="f87d4c121dedd51e95e3e29f848650ad3e6741b2ecbf167353f7de90ca011be0"
Feb 22 20:54:13 minikube kubelet[1384]: E0222 20:54:13.841216    1384 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"postgres\" with CrashLoopBackOff: \"back-off 2m40s restarting failed container=postgres pod=postgres-pod_default(8cc7dbd7-594c-4788-a010-1b3187361416)\"" pod="default/postgres-pod" podUID="8cc7dbd7-594c-4788-a010-1b3187361416"
Feb 22 20:54:24 minikube kubelet[1384]: I0222 20:54:24.819109    1384 scope.go:117] "RemoveContainer" containerID="f87d4c121dedd51e95e3e29f848650ad3e6741b2ecbf167353f7de90ca011be0"
Feb 22 20:54:24 minikube kubelet[1384]: E0222 20:54:24.819472    1384 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"postgres\" with CrashLoopBackOff: \"back-off 2m40s restarting failed container=postgres pod=postgres-pod_default(8cc7dbd7-594c-4788-a010-1b3187361416)\"" pod="default/postgres-pod" podUID="8cc7dbd7-594c-4788-a010-1b3187361416"
Feb 22 20:54:37 minikube kubelet[1384]: I0222 20:54:37.822057    1384 scope.go:117] "RemoveContainer" containerID="f87d4c121dedd51e95e3e29f848650ad3e6741b2ecbf167353f7de90ca011be0"
Feb 22 20:54:37 minikube kubelet[1384]: E0222 20:54:37.823063    1384 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"postgres\" with CrashLoopBackOff: \"back-off 2m40s restarting failed container=postgres pod=postgres-pod_default(8cc7dbd7-594c-4788-a010-1b3187361416)\"" pod="default/postgres-pod" podUID="8cc7dbd7-594c-4788-a010-1b3187361416"
Feb 22 20:54:52 minikube kubelet[1384]: I0222 20:54:52.799724    1384 scope.go:117] "RemoveContainer" containerID="f87d4c121dedd51e95e3e29f848650ad3e6741b2ecbf167353f7de90ca011be0"
Feb 22 20:54:52 minikube kubelet[1384]: E0222 20:54:52.800353    1384 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"postgres\" with CrashLoopBackOff: \"back-off 2m40s restarting failed container=postgres pod=postgres-pod_default(8cc7dbd7-594c-4788-a010-1b3187361416)\"" pod="default/postgres-pod" podUID="8cc7dbd7-594c-4788-a010-1b3187361416"
Feb 22 20:55:04 minikube kubelet[1384]: I0222 20:55:04.818433    1384 scope.go:117] "RemoveContainer" containerID="f87d4c121dedd51e95e3e29f848650ad3e6741b2ecbf167353f7de90ca011be0"
Feb 22 20:55:04 minikube kubelet[1384]: E0222 20:55:04.818687    1384 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"postgres\" with CrashLoopBackOff: \"back-off 2m40s restarting failed container=postgres pod=postgres-pod_default(8cc7dbd7-594c-4788-a010-1b3187361416)\"" pod="default/postgres-pod" podUID="8cc7dbd7-594c-4788-a010-1b3187361416"


==> storage-provisioner [1f09d80026fa] <==
I0220 17:17:43.656604       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F0220 17:18:13.662156       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: i/o timeout


==> storage-provisioner [8c347a69b2d8] <==
I0220 17:18:25.498333       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0220 17:18:25.524682       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0220 17:18:25.525705       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0220 17:18:42.966405       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0220 17:18:42.968668       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"0dd713d8-346a-4aba-84bb-84910f8eb096", APIVersion:"v1", ResourceVersion:"10523", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_2d77f038-a70e-487f-b56d-094e1b755a41 became leader
I0220 17:18:42.970199       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_2d77f038-a70e-487f-b56d-094e1b755a41!
I0220 17:18:43.090776       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_2d77f038-a70e-487f-b56d-094e1b755a41!

